{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection, Feature Engineering\n",
    "\n",
    "In this project, you will be tasked with building a plagiarism detector that examines an answer text file and performs binary classification; labeling that file as either plagiarized or not, depending on how similar that text file is to a provided, source text. \n",
    "\n",
    "Your first task will be to create some features that can then be used to train a classification model. This task will be broken down into a few discrete steps:\n",
    "\n",
    "* Clean and pre-process the data.\n",
    "* Define features for comparing the similarity of an answer text and a source text, and extract similarity features.\n",
    "* Select \"good\" features, by analyzing the correlations between different features.\n",
    "* Create train/test `.csv` files that hold the relevant features and class labels for train/test data points.\n",
    "\n",
    "In the _next_ notebook, Notebook 3, you'll use the features and `.csv` files you create in _this_ notebook to train a binary classification model in a SageMaker notebook instance.\n",
    "\n",
    "You'll be defining a few different similarity features, as outlined in [this paper](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c412841_developing-a-corpus-of-plagiarised-short-answers/developing-a-corpus-of-plagiarised-short-answers.pdf), which should help you build a robust plagiarism detector!\n",
    "\n",
    "To complete this notebook, you'll have to complete all given exercises and answer all the questions in this notebook.\n",
    "> All your tasks will be clearly labeled **EXERCISE** and questions as **QUESTION**.\n",
    "\n",
    "It will be up to you to decide on the features to include in your final training and test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data\n",
    "\n",
    "The cell below will download the necessary, project data and extract the files into the folder `data/`.\n",
    "\n",
    "This data is a slightly modified version of a dataset created by Paul Clough (Information Studies) and Mark Stevenson (Computer Science), at the University of Sheffield. You can read all about the data collection and corpus, at [their university webpage](https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html). \n",
    "\n",
    "> **Citation for data**: Clough, P. and Stevenson, M. Developing A Corpus of Plagiarised Short Answers, Language Resources and Evaluation: Special Issue on Plagiarism and Authorship Analysis, In Press. [Download]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-27 17:27:38--  https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c4147f9_data/data.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.89.214\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.89.214|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 113826 (111K) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>] 111.16K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-05-27 17:27:38 (4.67 MB/s) - ‘data.zip’ saved [113826/113826]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/.DS_Store          \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/data/\n",
      "  inflating: __MACOSX/data/._.DS_Store  \n",
      "  inflating: data/file_information.csv  \n",
      "  inflating: __MACOSX/data/._file_information.csv  \n",
      "  inflating: data/g0pA_taska.txt     \n",
      "  inflating: __MACOSX/data/._g0pA_taska.txt  \n",
      "  inflating: data/g0pA_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g0pA_taskb.txt  \n",
      "  inflating: data/g0pA_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g0pA_taskc.txt  \n",
      "  inflating: data/g0pA_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g0pA_taskd.txt  \n",
      "  inflating: data/g0pA_taske.txt     \n",
      "  inflating: __MACOSX/data/._g0pA_taske.txt  \n",
      "  inflating: data/g0pB_taska.txt     \n",
      "  inflating: __MACOSX/data/._g0pB_taska.txt  \n",
      "  inflating: data/g0pB_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g0pB_taskb.txt  \n",
      "  inflating: data/g0pB_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g0pB_taskc.txt  \n",
      "  inflating: data/g0pB_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g0pB_taskd.txt  \n",
      "  inflating: data/g0pB_taske.txt     \n",
      "  inflating: __MACOSX/data/._g0pB_taske.txt  \n",
      "  inflating: data/g0pC_taska.txt     \n",
      "  inflating: __MACOSX/data/._g0pC_taska.txt  \n",
      "  inflating: data/g0pC_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g0pC_taskb.txt  \n",
      "  inflating: data/g0pC_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g0pC_taskc.txt  \n",
      "  inflating: data/g0pC_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g0pC_taskd.txt  \n",
      "  inflating: data/g0pC_taske.txt     \n",
      "  inflating: __MACOSX/data/._g0pC_taske.txt  \n",
      "  inflating: data/g0pD_taska.txt     \n",
      "  inflating: __MACOSX/data/._g0pD_taska.txt  \n",
      "  inflating: data/g0pD_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g0pD_taskb.txt  \n",
      "  inflating: data/g0pD_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g0pD_taskc.txt  \n",
      "  inflating: data/g0pD_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g0pD_taskd.txt  \n",
      "  inflating: data/g0pD_taske.txt     \n",
      "  inflating: __MACOSX/data/._g0pD_taske.txt  \n",
      "  inflating: data/g0pE_taska.txt     \n",
      "  inflating: __MACOSX/data/._g0pE_taska.txt  \n",
      "  inflating: data/g0pE_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g0pE_taskb.txt  \n",
      "  inflating: data/g0pE_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g0pE_taskc.txt  \n",
      "  inflating: data/g0pE_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g0pE_taskd.txt  \n",
      "  inflating: data/g0pE_taske.txt     \n",
      "  inflating: __MACOSX/data/._g0pE_taske.txt  \n",
      "  inflating: data/g1pA_taska.txt     \n",
      "  inflating: __MACOSX/data/._g1pA_taska.txt  \n",
      "  inflating: data/g1pA_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g1pA_taskb.txt  \n",
      "  inflating: data/g1pA_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g1pA_taskc.txt  \n",
      "  inflating: data/g1pA_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g1pA_taskd.txt  \n",
      "  inflating: data/g1pA_taske.txt     \n",
      "  inflating: __MACOSX/data/._g1pA_taske.txt  \n",
      "  inflating: data/g1pB_taska.txt     \n",
      "  inflating: __MACOSX/data/._g1pB_taska.txt  \n",
      "  inflating: data/g1pB_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g1pB_taskb.txt  \n",
      "  inflating: data/g1pB_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g1pB_taskc.txt  \n",
      "  inflating: data/g1pB_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g1pB_taskd.txt  \n",
      "  inflating: data/g1pB_taske.txt     \n",
      "  inflating: __MACOSX/data/._g1pB_taske.txt  \n",
      "  inflating: data/g1pD_taska.txt     \n",
      "  inflating: __MACOSX/data/._g1pD_taska.txt  \n",
      "  inflating: data/g1pD_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g1pD_taskb.txt  \n",
      "  inflating: data/g1pD_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g1pD_taskc.txt  \n",
      "  inflating: data/g1pD_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g1pD_taskd.txt  \n",
      "  inflating: data/g1pD_taske.txt     \n",
      "  inflating: __MACOSX/data/._g1pD_taske.txt  \n",
      "  inflating: data/g2pA_taska.txt     \n",
      "  inflating: __MACOSX/data/._g2pA_taska.txt  \n",
      "  inflating: data/g2pA_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g2pA_taskb.txt  \n",
      "  inflating: data/g2pA_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g2pA_taskc.txt  \n",
      "  inflating: data/g2pA_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g2pA_taskd.txt  \n",
      "  inflating: data/g2pA_taske.txt     \n",
      "  inflating: __MACOSX/data/._g2pA_taske.txt  \n",
      "  inflating: data/g2pB_taska.txt     \n",
      "  inflating: __MACOSX/data/._g2pB_taska.txt  \n",
      "  inflating: data/g2pB_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g2pB_taskb.txt  \n",
      "  inflating: data/g2pB_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g2pB_taskc.txt  \n",
      "  inflating: data/g2pB_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g2pB_taskd.txt  \n",
      "  inflating: data/g2pB_taske.txt     \n",
      "  inflating: __MACOSX/data/._g2pB_taske.txt  \n",
      "  inflating: data/g2pC_taska.txt     \n",
      "  inflating: __MACOSX/data/._g2pC_taska.txt  \n",
      "  inflating: data/g2pC_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g2pC_taskb.txt  \n",
      "  inflating: data/g2pC_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g2pC_taskc.txt  \n",
      "  inflating: data/g2pC_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g2pC_taskd.txt  \n",
      "  inflating: data/g2pC_taske.txt     \n",
      "  inflating: __MACOSX/data/._g2pC_taske.txt  \n",
      "  inflating: data/g2pE_taska.txt     \n",
      "  inflating: __MACOSX/data/._g2pE_taska.txt  \n",
      "  inflating: data/g2pE_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g2pE_taskb.txt  \n",
      "  inflating: data/g2pE_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g2pE_taskc.txt  \n",
      "  inflating: data/g2pE_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g2pE_taskd.txt  \n",
      "  inflating: data/g2pE_taske.txt     \n",
      "  inflating: __MACOSX/data/._g2pE_taske.txt  \n",
      "  inflating: data/g3pA_taska.txt     \n",
      "  inflating: __MACOSX/data/._g3pA_taska.txt  \n",
      "  inflating: data/g3pA_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g3pA_taskb.txt  \n",
      "  inflating: data/g3pA_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g3pA_taskc.txt  \n",
      "  inflating: data/g3pA_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g3pA_taskd.txt  \n",
      "  inflating: data/g3pA_taske.txt     \n",
      "  inflating: __MACOSX/data/._g3pA_taske.txt  \n",
      "  inflating: data/g3pB_taska.txt     \n",
      "  inflating: __MACOSX/data/._g3pB_taska.txt  \n",
      "  inflating: data/g3pB_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g3pB_taskb.txt  \n",
      "  inflating: data/g3pB_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g3pB_taskc.txt  \n",
      "  inflating: data/g3pB_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g3pB_taskd.txt  \n",
      "  inflating: data/g3pB_taske.txt     \n",
      "  inflating: __MACOSX/data/._g3pB_taske.txt  \n",
      "  inflating: data/g3pC_taska.txt     \n",
      "  inflating: __MACOSX/data/._g3pC_taska.txt  \n",
      "  inflating: data/g3pC_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g3pC_taskb.txt  \n",
      "  inflating: data/g3pC_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g3pC_taskc.txt  \n",
      "  inflating: data/g3pC_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g3pC_taskd.txt  \n",
      "  inflating: data/g3pC_taske.txt     \n",
      "  inflating: __MACOSX/data/._g3pC_taske.txt  \n",
      "  inflating: data/g4pB_taska.txt     \n",
      "  inflating: __MACOSX/data/._g4pB_taska.txt  \n",
      "  inflating: data/g4pB_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g4pB_taskb.txt  \n",
      "  inflating: data/g4pB_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g4pB_taskc.txt  \n",
      "  inflating: data/g4pB_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g4pB_taskd.txt  \n",
      "  inflating: data/g4pB_taske.txt     \n",
      "  inflating: __MACOSX/data/._g4pB_taske.txt  \n",
      "  inflating: data/g4pC_taska.txt     \n",
      "  inflating: __MACOSX/data/._g4pC_taska.txt  \n",
      "  inflating: data/g4pC_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g4pC_taskb.txt  \n",
      "  inflating: data/g4pC_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g4pC_taskc.txt  \n",
      "  inflating: data/g4pC_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g4pC_taskd.txt  \n",
      "  inflating: data/g4pC_taske.txt     \n",
      "  inflating: __MACOSX/data/._g4pC_taske.txt  \n",
      "  inflating: data/g4pD_taska.txt     \n",
      "  inflating: __MACOSX/data/._g4pD_taska.txt  \n",
      "  inflating: data/g4pD_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g4pD_taskb.txt  \n",
      "  inflating: data/g4pD_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g4pD_taskc.txt  \n",
      "  inflating: data/g4pD_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g4pD_taskd.txt  \n",
      "  inflating: data/g4pD_taske.txt     \n",
      "  inflating: __MACOSX/data/._g4pD_taske.txt  \n",
      "  inflating: data/g4pE_taska.txt     \n",
      "  inflating: __MACOSX/data/._g4pE_taska.txt  \n",
      "  inflating: data/g4pE_taskb.txt     \n",
      "  inflating: __MACOSX/data/._g4pE_taskb.txt  \n",
      "  inflating: data/g4pE_taskc.txt     \n",
      "  inflating: __MACOSX/data/._g4pE_taskc.txt  \n",
      "  inflating: data/g4pE_taskd.txt     \n",
      "  inflating: __MACOSX/data/._g4pE_taskd.txt  \n",
      "  inflating: data/g4pE_taske.txt     \n",
      "  inflating: __MACOSX/data/._g4pE_taske.txt  \n",
      "  inflating: data/orig_taska.txt     \n",
      "  inflating: __MACOSX/data/._orig_taska.txt  \n",
      "  inflating: data/orig_taskb.txt     \n",
      "  inflating: data/orig_taskc.txt     \n",
      "  inflating: __MACOSX/data/._orig_taskc.txt  \n",
      "  inflating: data/orig_taskd.txt     \n",
      "  inflating: __MACOSX/data/._orig_taskd.txt  \n",
      "  inflating: data/orig_taske.txt     \n",
      "  inflating: __MACOSX/data/._orig_taske.txt  \n",
      "  inflating: data/test_info.csv      \n",
      "  inflating: __MACOSX/data/._test_info.csv  \n",
      "  inflating: __MACOSX/._data         \n"
     ]
    }
   ],
   "source": [
    "# NOTE:\n",
    "# you only need to run this cell if you have not yet downloaded the data\n",
    "# otherwise you may skip this cell or comment it out\n",
    "\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c4147f9_data/data.zip\n",
    "!unzip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plagiarism dataset is made of multiple text files; each of these files has characteristics that are is summarized in a `.csv` file named `file_information.csv`, which we can read in using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>heavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task Category\n",
       "0  g0pA_taska.txt    a      non\n",
       "1  g0pA_taskb.txt    b      cut\n",
       "2  g0pA_taskc.txt    c    light\n",
       "3  g0pA_taskd.txt    d    heavy\n",
       "4  g0pA_taske.txt    e      non"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = 'data/file_information.csv'\n",
    "plagiarism_df = pd.read_csv(csv_file)\n",
    "\n",
    "# print out the first few rows of data info\n",
    "plagiarism_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Plagiarism\n",
    "\n",
    "Each text file is associated with one **Task** (task A-E) and one **Category** of plagiarism, which you can see in the above DataFrame.\n",
    "\n",
    "###  Tasks, A-E\n",
    "\n",
    "Each text file contains an answer to one short question; these questions are labeled as tasks A-E. For example, Task A asks the question: \"What is inheritance in object oriented programming?\"\n",
    "\n",
    "### Categories of plagiarism \n",
    "\n",
    "Each text file has an associated plagiarism label/category:\n",
    "\n",
    "**1. Plagiarized categories: `cut`, `light`, and `heavy`.**\n",
    "* These categories represent different levels of plagiarized answer texts. `cut` answers copy directly from a source text, `light` answers are based on the source text but include some light rephrasing, and `heavy` answers are based on the source text, but *heavily* rephrased (and will likely be the most challenging kind of plagiarism to detect).\n",
    "     \n",
    "**2. Non-plagiarized category: `non`.** \n",
    "* `non` indicates that an answer is not plagiarized; the Wikipedia source text is not used to create this answer.\n",
    "    \n",
    "**3. Special, source text category: `orig`.**\n",
    "* This is a specific category for the original, Wikipedia source text. We will use these files only for comparison purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Process the Data\n",
    "\n",
    "In the next few cells, you'll be tasked with creating a new DataFrame of desired information about all of the files in the `data/` directory. This will prepare the data for feature extraction and for training a binary, plagiarism classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Convert categorical to numerical data\n",
    "\n",
    "You'll notice that the `Category` column in the data, contains string or categorical values, and to prepare these for feature extraction, we'll want to convert these into numerical values. Additionally, our goal is to create a binary classifier and so we'll need a binary class label that indicates whether an answer text is plagiarized (1) or not (0). Complete the below function `numerical_dataframe` that reads in a `file_information.csv` file by name, and returns a *new* DataFrame with a numerical `Category` column and a new `Class` column that labels each answer as plagiarized or not. \n",
    "\n",
    "Your function should return a new DataFrame with the following properties:\n",
    "\n",
    "* 4 columns: `File`, `Task`, `Category`, `Class`. The `File` and `Task` columns can remain unchanged from the original `.csv` file.\n",
    "* Convert all `Category` labels to numerical labels according to the following rules (a higher value indicates a higher degree of plagiarism):\n",
    "    * 0 = `non`\n",
    "    * 1 = `heavy`\n",
    "    * 2 = `light`\n",
    "    * 3 = `cut`\n",
    "    * -1 = `orig`, this is a special value that indicates an original file.\n",
    "* For the new `Class` column\n",
    "    * Any answer text that is not plagiarized (`non`) should have the class label `0`. \n",
    "    * Any plagiarized answer texts should have the class label `1`. \n",
    "    * And any `orig` texts will have a special label `-1`. \n",
    "\n",
    "### Expected output\n",
    "\n",
    "After running your function, you should get a DataFrame with rows that looks like the following: \n",
    "```\n",
    "\n",
    "        File\t     Task  Category  Class\n",
    "0\tg0pA_taska.txt\ta\t  0   \t0\n",
    "1\tg0pA_taskb.txt\tb\t  3   \t1\n",
    "2\tg0pA_taskc.txt\tc\t  2   \t1\n",
    "3\tg0pA_taskd.txt\td\t  1   \t1\n",
    "4\tg0pA_taske.txt\te\t  0\t   0\n",
    "...\n",
    "...\n",
    "99   orig_taske.txt    e     -1      -1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_labels(name):\n",
    "    if name == 'non':\n",
    "        return 0\n",
    "    elif name == 'heavy':\n",
    "        return 1\n",
    "    elif name == 'light':\n",
    "        return 2\n",
    "    elif name == 'cut':\n",
    "        return 3\n",
    "    elif name == 'orig':\n",
    "        return -1\n",
    "\n",
    "# Read in a csv file and return a transformed dataframe\n",
    "def numerical_dataframe(csv_file='data/file_information.csv'):\n",
    "    '''Reads in a csv file which is assumed to have `File`, `Category` and `Task` columns.\n",
    "       This function does two things: \n",
    "       1) converts `Category` column values to numerical values \n",
    "       2) Adds a new, numerical `Class` label column.\n",
    "       The `Class` column will label plagiarized answers as 1 and non-plagiarized as 0.\n",
    "       Source texts have a special label, -1.\n",
    "       :param csv_file: The directory for the file_information.csv file\n",
    "       :return: A dataframe with numerical categories and a new `Class` label column'''\n",
    "    \n",
    "    # your code here\n",
    "    text = pd.read_csv(csv_file)\n",
    "    category_dict = {\"non\":0,\"heavy\":1,\"light\":2,\"cut\":3,\"orig\":-1}\n",
    "    text['Category'] = text['Category'].apply(lambda t: category_labels(t))\n",
    "    text['Class'] = [0 if z==0 else -1 if z==-1 else 1 for z in text['Category']]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "Below are a couple of test cells. The first is an informal test where you can check that your code is working as expected by calling your function and printing out the returned result.\n",
    "\n",
    "The **second** cell below is a more rigorous test cell. The goal of a cell like this is to ensure that your code is working as expected, and to form any variables that might be used in _later_ tests/code, in this case, the data frame, `transformed_df`.\n",
    "\n",
    "> The cells in this notebook should be run in chronological order (the order they appear in the notebook). This is especially important for test cells.\n",
    "\n",
    "Often, later cells rely on the functions, imports, or variables defined in earlier cells. For example, some tests rely on previous tests to work.\n",
    "\n",
    "These tests do not test all cases, but they are a great way to check that you are on the right track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>g0pB_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g0pB_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>g0pB_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g0pB_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g0pB_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task  Category  Class\n",
       "0  g0pA_taska.txt    a         0      0\n",
       "1  g0pA_taskb.txt    b         3      1\n",
       "2  g0pA_taskc.txt    c         2      1\n",
       "3  g0pA_taskd.txt    d         1      1\n",
       "4  g0pA_taske.txt    e         0      0\n",
       "5  g0pB_taska.txt    a         0      0\n",
       "6  g0pB_taskb.txt    b         0      0\n",
       "7  g0pB_taskc.txt    c         3      1\n",
       "8  g0pB_taskd.txt    d         2      1\n",
       "9  g0pB_taske.txt    e         1      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# informal testing, print out the results of a called function\n",
    "# create new `transformed_df`\n",
    "transformed_df = numerical_dataframe(csv_file ='data/file_information.csv')\n",
    "\n",
    "# check work\n",
    "# check that all categories of plagiarism have a class label = 1\n",
    "transformed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n",
      "\n",
      "Example data: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task  Category  Class\n",
       "0  g0pA_taska.txt    a         0      0\n",
       "1  g0pA_taskb.txt    b         3      1\n",
       "2  g0pA_taskc.txt    c         2      1\n",
       "3  g0pA_taskd.txt    d         1      1\n",
       "4  g0pA_taske.txt    e         0      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cell that creates `transformed_df`, if tests are passed\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "# importing tests\n",
    "import problem_unittests as tests\n",
    "\n",
    "# test numerical_dataframe function\n",
    "tests.test_numerical_df(numerical_dataframe)\n",
    "\n",
    "# if above test is passed, create NEW `transformed_df`\n",
    "transformed_df = numerical_dataframe(csv_file ='data/file_information.csv')\n",
    "\n",
    "# check work\n",
    "print('\\nExample data: ')\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing & Splitting Data\n",
    "\n",
    "Recall that the goal of this project is to build a plagiarism classifier. At it's heart, this task is a comparison text; one that looks at a given answer and a source text, compares them and predicts whether an answer has plagiarized from the source. To effectively do this comparison, and train a classifier we'll need to do a few more things: pre-process all of our text data and prepare the text files (in this case, the 95 answer files and 5 original source files) to be easily compared, and split our data into a `train` and `test` set that can be used to train a classifier and evaluate it, respectively. \n",
    "\n",
    "To this end, you've been provided code that adds  additional information to your `transformed_df` from above. The next two cells need not be changed; they add two additional columns to the `transformed_df`:\n",
    "\n",
    "1. A `Text` column; this holds all the lowercase text for a `File`, with extraneous punctuation removed.\n",
    "2. A `Datatype` column; this is a string value `train`, `test`, or `orig` that labels a data point as part of our train or test set\n",
    "\n",
    "The details of how these additional columns are created can be found in the `helpers.py` file in the project directory. You're encouraged to read through that file to see exactly how text is processed and how data is split.\n",
    "\n",
    "Run the cells below to get a `complete_df` that has all the information you need to proceed with plagiarism detection and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>inheritance is a basic concept of object orien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>pagerank is a link analysis algorithm used by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>the vector space model also called term vector...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bayes theorem was names after rev thomas bayes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dynamic programming is an algorithm design tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task  Category  Class  \\\n",
       "0  g0pA_taska.txt    a         0      0   \n",
       "1  g0pA_taskb.txt    b         3      1   \n",
       "2  g0pA_taskc.txt    c         2      1   \n",
       "3  g0pA_taskd.txt    d         1      1   \n",
       "4  g0pA_taske.txt    e         0      0   \n",
       "\n",
       "                                                Text  \n",
       "0  inheritance is a basic concept of object orien...  \n",
       "1  pagerank is a link analysis algorithm used by ...  \n",
       "2  the vector space model also called term vector...  \n",
       "3  bayes theorem was names after rev thomas bayes...  \n",
       "4  dynamic programming is an algorithm design tec...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import helpers \n",
    "\n",
    "# create a text column \n",
    "text_df = helpers.create_text_column(transformed_df)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed text:\n",
      "\n",
      " inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n"
     ]
    }
   ],
   "source": [
    "# after running the cell above\n",
    "# check out the processed text for a single file, by row index\n",
    "row_idx = 0 # feel free to change this index\n",
    "\n",
    "sample_text = text_df.iloc[0]['Text']\n",
    "\n",
    "print('Sample processed text:\\n\\n', sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "\n",
    "The next cell will add a `Datatype` column to a given DataFrame to indicate if the record is: \n",
    "* `train` - Training data, for model training.\n",
    "* `test` - Testing data, for model evaluation.\n",
    "* `orig` - The task's original answer from wikipedia.\n",
    "\n",
    "### Stratified sampling\n",
    "\n",
    "The given code uses a helper function which you can view in the `helpers.py` file in the main project directory. This implements [stratified random sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to randomly split data by task & plagiarism amount. Stratified sampling ensures that we get training and test data that is fairly evenly distributed across task & plagiarism combinations. Approximately 26% of the data is held out for testing and 74% of the data is used for training.\n",
    "\n",
    "The function **train_test_dataframe** takes in a DataFrame that it assumes has `Task` and `Category` columns, and, returns a modified frame that indicates which `Datatype` (train, test, or orig) a file falls into. This sampling will change slightly based on a passed in *random_seed*. Due to a small sample size, this stratified random sampling will provide more stable results for a binary plagiarism classifier. Stability here is smaller *variance* in the accuracy of classifier, given a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Task</th>\n",
       "      <th>Category</th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "      <th>Datatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g0pA_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>inheritance is a basic concept of object orien...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g0pA_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>pagerank is a link analysis algorithm used by ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g0pA_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>the vector space model also called term vector...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g0pA_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bayes theorem was names after rev thomas bayes...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g0pA_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dynamic programming is an algorithm design tec...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>g0pB_taska.txt</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>inheritance is a basic concept in object orien...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>g0pB_taskb.txt</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pagerank pr refers to both the concept and the...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>g0pB_taskc.txt</td>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>vector space model is an algebraic model for r...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>g0pB_taskd.txt</td>\n",
       "      <td>d</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>bayes theorem relates the conditional and marg...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>g0pB_taske.txt</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dynamic programming is a method for solving ma...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             File Task  Category  Class  \\\n",
       "0  g0pA_taska.txt    a         0      0   \n",
       "1  g0pA_taskb.txt    b         3      1   \n",
       "2  g0pA_taskc.txt    c         2      1   \n",
       "3  g0pA_taskd.txt    d         1      1   \n",
       "4  g0pA_taske.txt    e         0      0   \n",
       "5  g0pB_taska.txt    a         0      0   \n",
       "6  g0pB_taskb.txt    b         0      0   \n",
       "7  g0pB_taskc.txt    c         3      1   \n",
       "8  g0pB_taskd.txt    d         2      1   \n",
       "9  g0pB_taske.txt    e         1      1   \n",
       "\n",
       "                                                Text Datatype  \n",
       "0  inheritance is a basic concept of object orien...    train  \n",
       "1  pagerank is a link analysis algorithm used by ...     test  \n",
       "2  the vector space model also called term vector...    train  \n",
       "3  bayes theorem was names after rev thomas bayes...    train  \n",
       "4  dynamic programming is an algorithm design tec...    train  \n",
       "5  inheritance is a basic concept in object orien...    train  \n",
       "6  pagerank pr refers to both the concept and the...    train  \n",
       "7  vector space model is an algebraic model for r...     test  \n",
       "8  bayes theorem relates the conditional and marg...    train  \n",
       "9  dynamic programming is a method for solving ma...     test  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 1 # can change; set for reproducibility\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import helpers\n",
    "\n",
    "# create new df with Datatype (train, test, orig) column\n",
    "# pass in `text_df` from above to create a complete dataframe, with all the information you need\n",
    "complete_df = helpers.train_test_dataframe(text_df, random_seed=random_seed)\n",
    "\n",
    "# check results\n",
    "complete_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Plagiarism\n",
    "\n",
    "Now that you've prepared this data and created a `complete_df` of information, including the text and class associated with each file, you can move on to the task of extracting similarity features that will be useful for plagiarism classification. \n",
    "\n",
    "> Note: The following code exercises, assume that the `complete_df` as it exists now, will **not** have its existing columns modified. \n",
    "\n",
    "The `complete_df` should always include the columns: `['File', 'Task', 'Category', 'Class', 'Text', 'Datatype']`. You can add additional columns, and you can create any new DataFrames you need by copying the parts of the `complete_df` as long as you do not modify the existing values, directly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Similarity Features \n",
    "\n",
    "One of the ways we might go about detecting plagiarism, is by computing **similarity features** that measure how similar a given answer text is as compared to the original wikipedia source text (for a specific task, a-e). The similarity features you will use are informed by [this paper on plagiarism detection](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c412841_developing-a-corpus-of-plagiarised-short-answers/developing-a-corpus-of-plagiarised-short-answers.pdf). \n",
    "> In this paper, researchers created features called **containment** and **longest common subsequence**. \n",
    "\n",
    "Using these features as input, you will train a model to distinguish between plagiarized and not-plagiarized text files.\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Let's talk a bit more about the features we want to include in a plagiarism detection model and how to calculate such features. In the following explanations, I'll refer to a submitted text file as a **Student Answer Text (A)** and the original, wikipedia source file (that we want to compare that answer to) as the **Wikipedia Source Text (S)**.\n",
    "\n",
    "### Containment\n",
    "\n",
    "Your first task will be to create **containment features**. To understand containment, let's first revisit a definition of [n-grams](https://en.wikipedia.org/wiki/N-gram). An *n-gram* is a sequential word grouping. For example, in a line like \"bayes rule gives us a way to combine prior knowledge with new information,\" a 1-gram is just one word, like \"bayes.\" A 2-gram might be \"bayes rule\" and a 3-gram might be \"combine prior knowledge.\"\n",
    "\n",
    "> Containment is defined as the **intersection** of the n-gram word count of the Wikipedia Source Text (S) with the n-gram word count of the Student  Answer Text (S) *divided* by the n-gram word count of the Student Answer Text.\n",
    "\n",
    "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$\n",
    "\n",
    "If the two texts have no n-grams in common, the containment will be 0, but if _all_ their n-grams intersect then the containment will be 1. Intuitively, you can see how having longer n-gram's in common, might be an indication of cut-and-paste plagiarism. In this project, it will be up to you to decide on the appropriate `n` or several `n`'s to use in your final model.\n",
    "\n",
    "### EXERCISE: Create containment features\n",
    "\n",
    "Given the `complete_df` that you've created, you should have all the information you need to compare any Student  Answer Text (A) with its appropriate Wikipedia Source Text (S). An answer for task A should be compared to the source text for task A, just as answers to tasks B, C, D, and E should be compared to the corresponding original source text.\n",
    "\n",
    "In this exercise, you'll complete the function, `calculate_containment` which calculates containment based upon the following parameters:\n",
    "* A given DataFrame, `df` (which is assumed to be the `complete_df` from above)\n",
    "* An `answer_filename`, such as 'g0pB_taskd.txt' \n",
    "* An n-gram length, `n`\n",
    "\n",
    "### Containment calculation\n",
    "\n",
    "The general steps to complete this function are as follows:\n",
    "1. From *all* of the text files in a given `df`, create an array of n-gram counts; it is suggested that you use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for this purpose.\n",
    "2. Get the processed answer and source texts for the given `answer_filename`.\n",
    "3. Calculate the containment between an answer and source text according to the following equation.\n",
    "\n",
    "    >$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$\n",
    "    \n",
    "4. Return that containment value.\n",
    "\n",
    "You are encouraged to write any helper functions that you need to complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ngram containment for one answer file/source file pair in a df\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def calculate_containment(df, n, answer_filename):\n",
    "    '''Calculates the containment between a given answer text and its associated source text.\n",
    "       This function creates a count of ngrams (of a size, n) for each text file in our data.\n",
    "       Then calculates the containment by finding the ngram count for a given answer text, \n",
    "       and its associated source text, and calculating the normalized intersection of those counts.\n",
    "       :param df: A dataframe with columns,\n",
    "           'File', 'Task', 'Category', 'Class', 'Text', and 'Datatype'\n",
    "       :param n: An integer that defines the ngram size\n",
    "       :param answer_filename: A filename for an answer text in the df, ex. 'g0pB_taskd.txt'\n",
    "       :return: A single containment value that represents the similarity\n",
    "           between an answer text and its source text.\n",
    "    '''\n",
    "\n",
    "        \n",
    "    vectorizer = CountVectorizer( analyzer='word',  ngram_range = (n,n))\n",
    "    \n",
    "    c = pd.Index(df['File'] )\n",
    "    req = c.get_loc(answer_filename)\n",
    "    \n",
    "    answer_text_task = df.iloc[req,:].Task\n",
    "    answer_text = df.iloc[req ,:].Text\n",
    "    \n",
    "    original_text = df[ df['Category'] == -1   ]\n",
    "    req_original = original_text[original_text['Task'] == answer_text_task ]\n",
    "    original_text = req_original['Text'].values[0] \n",
    "    \n",
    "    print(answer_text_task)\n",
    "    print(answer_text)\n",
    "    print(original_text)\n",
    "    \n",
    "    pair = [answer_text , original_text]\n",
    "    trans_array = (vectorizer.fit_transform(pair) ) .toarray()\n",
    "    \n",
    "    \n",
    "    num = trans_array[0,:].shape[0]\n",
    "    x  = 0\n",
    "    for i in range(num):\n",
    "        x += min(trans_array[0,i] , trans_array[1,i] )\n",
    "                \n",
    "    d = trans_array[0,:].sum()\n",
    "    ans  = x / d\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "After you've implemented the containment function, you can test out its behavior. \n",
    "\n",
    "The cell below iterates through the first few files, and calculates the original category _and_ containment values for a specified n and file.\n",
    "\n",
    ">If you've implemented this correctly, you should see that the non-plagiarized have low or close to 0 containment values and that plagiarized examples have higher containment values, closer to 1.\n",
    "\n",
    "Note what happens when you change the value of n. I recommend applying your code to multiple files and comparing the resultant containment values. You should see that the highest containment values correspond to files with the highest category (`cut`) of plagiarism level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "Original category values: \n",
      " [0, 3, 2, 1, 0]\n",
      "\n",
      "3-gram containment values: \n",
      " [0.009345794392523364, 0.9641025641025641, 0.6136363636363636, 0.15675675675675677, 0.031746031746031744]\n"
     ]
    }
   ],
   "source": [
    "# select a value for n\n",
    "n = 3\n",
    "\n",
    "# indices for first few files\n",
    "test_indices = range(5)\n",
    "\n",
    "# iterate through files and calculate containment\n",
    "category_vals = []\n",
    "containment_vals = []\n",
    "for i in test_indices:\n",
    "    # get level of plagiarism for a given file index\n",
    "    category_vals.append(complete_df.loc[i, 'Category'])\n",
    "    # calculate containment for given file and n\n",
    "    filename = complete_df.loc[i, 'File']\n",
    "    c = calculate_containment(complete_df, n, filename)\n",
    "    containment_vals.append(c)\n",
    "\n",
    "# print out result, does it make sense?\n",
    "print('Original category values: \\n', category_vals)\n",
    "print()\n",
    "print(str(n)+'-gram containment values: \\n', containment_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# run this test cell\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test containment calculation\n",
    "# params: complete_df from before, and containment function\n",
    "tests.test_containment(complete_df, calculate_containment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1: Why can we calculate containment features across *all* data (training & test), prior to splitting the DataFrame for modeling? That is, what about the containment calculation means that the test and training data do not influence each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "We can calculate containment for whole data because the test and train should be proprocessed on the same level before modelling. Containment is not dependent on test and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Longest Common Subsequence\n",
    "\n",
    "Containment a good way to find overlap in word usage between two documents; it may help identify cases of cut-and-paste as well as paraphrased levels of plagiarism. Since plagiarism is a fairly complex task with varying levels, it's often useful to include other measures of similarity. The paper also discusses a feature called **longest common subsequence**.\n",
    "\n",
    "> The longest common subsequence is the longest string of words (or letters) that are *the same* between the Wikipedia Source Text (S) and the Student Answer Text (A). This value is also normalized by dividing by the total number of words (or letters) in the  Student Answer Text. \n",
    "\n",
    "In this exercise, we'll ask you to calculate the longest common subsequence of words between two texts.\n",
    "\n",
    "### EXERCISE: Calculate the longest common subsequence\n",
    "\n",
    "Complete the function `lcs_norm_word`; this should calculate the *longest common subsequence* of words between a Student Answer Text and corresponding Wikipedia Source Text. \n",
    "\n",
    "It may be helpful to think of this in a concrete example. A Longest Common Subsequence (LCS) problem may look as follows:\n",
    "* Given two texts: text A (answer text) of length n, and string S (original source text) of length m. Our goal is to produce their longest common subsequence of words: the longest sequence of words that appear left-to-right in both texts (though the words don't have to be in continuous order).\n",
    "* Consider:\n",
    "    * A = \"i think pagerank is a link analysis algorithm used by google that uses a system of weights attached to each element of a hyperlinked set of documents\"\n",
    "    * S = \"pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents\"\n",
    "\n",
    "* In this case, we can see that the start of each sentence of fairly similar, having overlap in the sequence of words, \"pagerank is a link analysis algorithm used by\" before diverging slightly. Then we **continue moving left -to-right along both texts** until we see the next common sequence; in this case it is only one word, \"google\". Next we find \"that\" and \"a\" and finally the same ending \"to each element of a hyperlinked set of documents\".\n",
    "* Below, is a clear visual of how these sequences were found, sequentially, in each text.\n",
    "\n",
    "<img src='notebook_ims/common_subseq_words.png' width=40% />\n",
    "\n",
    "* Now, those words appear in left-to-right order in each document, sequentially, and even though there are some words in between, we count this as the longest common subsequence between the two texts. \n",
    "* If I count up each word that I found in common I get the value 20. **So, LCS has length 20**. \n",
    "* Next, to normalize this value, divide by the total length of the student answer; in this example that length is only 27. **So, the function `lcs_norm_word` should return the value `20/27` or about `0.7408`.**\n",
    "\n",
    "In this way, LCS is a great indicator of cut-and-paste plagiarism or if someone has referenced the same source text multiple times in an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCS, dynamic programming\n",
    "\n",
    "If you read through the scenario above, you can see that this algorithm depends on looking at two texts and comparing them word by word. You can solve this problem in multiple ways. First, it may be useful to `.split()` each text into lists of comma separated words to compare. Then, you can iterate through each word in the texts and compare them, adding to your value for LCS as you go. \n",
    "\n",
    "The method I recommend for implementing an efficient LCS algorithm is: using a matrix and dynamic programming. **Dynamic programming** is all about breaking a larger problem into a smaller set of subproblems, and building up a complete result without having to repeat any subproblems. \n",
    "\n",
    "This approach assumes that you can split up a large LCS task into a combination of smaller LCS tasks. Let's look at a simple example that compares letters:\n",
    "\n",
    "* A = \"ABCD\"\n",
    "* S = \"BD\"\n",
    "\n",
    "We can see right away that the longest subsequence of _letters_ here is 2 (B and D are in sequence in both strings). And we can calculate this by looking at relationships between each letter in the two strings, A and S.\n",
    "\n",
    "Here, I have a matrix with the letters of A on top and the letters of S on the left side:\n",
    "\n",
    "<img src='notebook_ims/matrix_1.png' width=40% />\n",
    "\n",
    "This starts out as a matrix that has as many columns and rows as letters in the strings S and O **+1** additional row and column, filled with zeros on the top and left sides. So, in this case, instead of a 2x4 matrix it is a 3x5.\n",
    "\n",
    "Now, we can fill this matrix up by breaking it into smaller LCS problems. For example, let's first look at the shortest substrings: the starting letter of A and S. We'll first ask, what is the Longest Common Subsequence between these two letters \"A\" and \"B\"? \n",
    "\n",
    "**Here, the answer is zero and we fill in the corresponding grid cell with that value.**\n",
    "\n",
    "<img src='notebook_ims/matrix_2.png' width=30% />\n",
    "\n",
    "Then, we ask the next question, what is the LCS between \"AB\" and \"B\"?\n",
    "\n",
    "**Here, we have a match, and can fill in the appropriate value 1**.\n",
    "\n",
    "<img src='notebook_ims/matrix_3_match.png' width=25% />\n",
    "\n",
    "If we continue, we get to a final matrix that looks as follows, with a **2** in the bottom right corner.\n",
    "\n",
    "<img src='notebook_ims/matrix_6_complete.png' width=25% />\n",
    "\n",
    "The final LCS will be that value **2** *normalized* by the number of n-grams in A. So, our normalized value is 2/4 = **0.5**.\n",
    "\n",
    "### The matrix rules\n",
    "\n",
    "One thing to notice here is that, you can efficiently fill up this matrix one cell at a time. Each grid cell only depends on the values in the grid cells that are directly on top and to the left of it, or on the diagonal/top-left. The rules are as follows:\n",
    "* Start with a matrix that has one extra row and column of zeros.\n",
    "* As you traverse your string:\n",
    "    * If there is a match, fill that grid cell with the value to the top-left of that cell *plus* one. So, in our case, when we found a matching B-B, we added +1 to the value in the top-left of the matching cell, 0.\n",
    "    * If there is not a match, take the *maximum* value from either directly to the left or the top cell, and carry that value over to the non-match cell.\n",
    "\n",
    "<img src='notebook_ims/matrix_rules.png' width=50% />\n",
    "\n",
    "After completely filling the matrix, **the bottom-right cell will hold the non-normalized LCS value**.\n",
    "\n",
    "This matrix treatment can be applied to a set of words instead of letters. Your function should apply this to the words in two texts and return the normalized LCS value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized LCS given an answer text and a source text\n",
    "def lcs_norm_word(answer_text, source_text):\n",
    "    '''Computes the longest common subsequence of words in two texts; returns a normalized value.\n",
    "       :param answer_text: The pre-processed text for an answer text\n",
    "       :param source_text: The pre-processed text for an answer's associated source text\n",
    "       :return: A normalized LCS value'''\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    mat = [[0 for i in range(len(source_text) + 1)] for j in range(len(answer_text) + 1)]\n",
    "    B = source_text.split()\n",
    "    A = answer_text.split()\n",
    "    mat = [[0 for i in range(len(B) + 1)] for j in range(len(A) + 1)]\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B)):\n",
    "            if A[i] == B[j]:\n",
    "                mat[i+1][j+1] = mat[i][j] + 1\n",
    "            else:\n",
    "                mat[i+1][j+1] = max(mat[i][j+1], mat[i+1][j])\n",
    "    return(mat[-1][-1])/len(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "Let's start by testing out your code on the example given in the initial description.\n",
    "\n",
    "In the below cell, we have specified strings A (answer text) and S (original source text). We know that these texts have 20 words in common and the submitted answer is 27 words long, so the normalized, longest common subsequence should be 20/27.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS =  0.7407407407407407\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run the test scenario from above\n",
    "# does your function return the expected value?\n",
    "\n",
    "A = \"i think pagerank is a link analysis algorithm used by google that uses a system of weights attached to each element of a hyperlinked set of documents\"\n",
    "S = \"pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents\"\n",
    "\n",
    "# calculate LCS\n",
    "lcs = lcs_norm_word(A, S)\n",
    "print('LCS = ', lcs)\n",
    "\n",
    "\n",
    "# expected value test\n",
    "assert lcs==20/27., \"Incorrect LCS value, expected about 0.7408, got \"+str(lcs)\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell runs a more rigorous test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# run test cell\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test lcs implementation\n",
    "# params: complete_df from before, and lcs_norm_word function\n",
    "tests.test_lcs(complete_df, lcs_norm_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, take a look at a few resultant values for `lcs_norm_word`. Just like before, you should see that higher values correspond to higher levels of plagiarism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original category values: \n",
      " [0, 3, 2, 1, 0]\n",
      "\n",
      "Normalized LCS values: \n",
      " [0.1917808219178082, 0.8207547169811321, 0.8464912280701754, 0.3160621761658031, 0.24257425742574257]\n"
     ]
    }
   ],
   "source": [
    "# test on your own\n",
    "test_indices = range(5) # look at first few files\n",
    "\n",
    "category_vals = []\n",
    "lcs_norm_vals = []\n",
    "# iterate through first few docs and calculate LCS\n",
    "for i in test_indices:\n",
    "    category_vals.append(complete_df.loc[i, 'Category'])\n",
    "    # get texts to compare\n",
    "    answer_text = complete_df.loc[i, 'Text'] \n",
    "    task = complete_df.loc[i, 'Task']\n",
    "    # we know that source texts have Class = -1\n",
    "    orig_rows = complete_df[(complete_df['Class'] == -1)]\n",
    "    orig_row = orig_rows[(orig_rows['Task'] == task)]\n",
    "    source_text = orig_row['Text'].values[0]\n",
    "    \n",
    "    # calculate lcs\n",
    "    lcs_val = lcs_norm_word(answer_text, source_text)\n",
    "    lcs_norm_vals.append(lcs_val)\n",
    "\n",
    "# print out result, does it make sense?\n",
    "print('Original category values: \\n', category_vals)\n",
    "print()\n",
    "print('Normalized LCS values: \\n', lcs_norm_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create All Features\n",
    "\n",
    "Now that you've completed the feature calculation functions, it's time to actually create multiple features and decide on which ones to use in your final model! In the below cells, you're provided two helper functions to help you create multiple features and store those in a DataFrame, `features_df`.\n",
    "\n",
    "### Creating multiple containment features\n",
    "\n",
    "Your completed `calculate_containment` function will be called in the next cell, which defines the helper function `create_containment_features`. \n",
    "\n",
    "> This function returns a list of containment features, calculated for a given `n` and for *all* files in a df (assumed to the the `complete_df`).\n",
    "\n",
    "For our original files, the containment value is set to a special value, -1.\n",
    "\n",
    "This function gives you the ability to easily create several containment features, of different n-gram lengths, for each of our text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Function returns a list of containment features, calculated for a given n \n",
    "# Should return a list of length 100 for all files in a complete_df\n",
    "def create_containment_features(df, n, column_name=None):\n",
    "    \n",
    "    containment_values = []\n",
    "    \n",
    "    if(column_name==None):\n",
    "        column_name = 'c_'+str(n) # c_1, c_2, .. c_n\n",
    "    \n",
    "    # iterates through dataframe rows\n",
    "    for i in df.index:\n",
    "        file = df.loc[i, 'File']\n",
    "        # Computes features using calculate_containment function\n",
    "        if df.loc[i,'Category'] > -1:\n",
    "            c = calculate_containment(df, n, file)\n",
    "            containment_values.append(c)\n",
    "        # Sets value to -1 for original tasks \n",
    "        else:\n",
    "            containment_values.append(-1)\n",
    "    \n",
    "    print(str(n)+'-gram containment features created!')\n",
    "    return containment_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LCS features\n",
    "\n",
    "Below, your complete `lcs_norm_word` function is used to create a list of LCS features for all the answer files in a given DataFrame (again, this assumes you are passing in the `complete_df`. It assigns a special value for our original, source files, -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Function creates lcs feature and add it to the dataframe\n",
    "def create_lcs_features(df, column_name='lcs_word'):\n",
    "    \n",
    "    lcs_values = []\n",
    "    \n",
    "    # iterate through files in dataframe\n",
    "    for i in df.index:\n",
    "        # Computes LCS_norm words feature using function above for answer tasks\n",
    "        if df.loc[i,'Category'] > -1:\n",
    "            # get texts to compare\n",
    "            answer_text = df.loc[i, 'Text'] \n",
    "            task = df.loc[i, 'Task']\n",
    "            # we know that source texts have Class = -1\n",
    "            orig_rows = df[(df['Class'] == -1)]\n",
    "            orig_row = orig_rows[(orig_rows['Task'] == task)]\n",
    "            source_text = orig_row['Text'].values[0]\n",
    "\n",
    "            # calculate lcs\n",
    "            lcs = lcs_norm_word(answer_text, source_text)\n",
    "            lcs_values.append(lcs)\n",
    "        # Sets to -1 for original tasks \n",
    "        else:\n",
    "            lcs_values.append(-1)\n",
    "\n",
    "    print('LCS features created!')\n",
    "    return lcs_values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Create a features DataFrame by selecting an `ngram_range`\n",
    "\n",
    "The paper suggests calculating the following features: containment *1-gram to 5-gram* and *longest common subsequence*. \n",
    "> In this exercise, you can choose to create even more features, for example from *1-gram to 7-gram* containment features and *longest common subsequence*. \n",
    "\n",
    "You'll want to create at least 6 features to choose from as you think about which to give to your final, classification model. Defining and comparing at least 6 different features allows you to discard any features that seem redundant, and choose to use the best features for your final model!\n",
    "\n",
    "In the below cell **define an n-gram range**; these will be the n's you use to create n-gram containment features. The rest of the feature creation code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "1-gram containment features created!\n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "2-gram containment features created!\n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "3-gram containment features created!\n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "4-gram containment features created!\n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "5-gram containment features created!\n",
      "a\n",
      "inheritance is a basic concept of object oriented programming where the basic idea is to create new classes that add extra detail to existing classes this is done by allowing the new classes to reuse the methods and variables of the existing classes and new methods and classes are added to specialise the new class inheritance models the is kind of relationship between entities or objects  for example postgraduates and undergraduates are both kinds of student this kind of relationship can be visualised as a tree structure where student would be the more general root node and both postgraduate and undergraduate would be more specialised extensions of the student node or the child nodes  in this relationship student would be known as the superclass or parent class whereas  postgraduate would be known as the subclass or child class because the postgraduate class extends the student class  inheritance can occur on several layers where if visualised would display a larger tree structure for example we could further extend the postgraduate node by adding two extra extended classes to it called  msc student and phd student as both these types of student are kinds of postgraduate student this would mean that both the msc student and phd student classes would inherit methods and variables from both the postgraduate and student classes  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google  the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank  other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model also called term vector model is an algebraic model used to represent text documents as well as any objects in general as vectors of identifiers it is used in information retrieval and was first used in the smart information retrieval system  a document is represented as a vector and each dimension corresponds to a separate term if a term appears in the document then its value in the vector is non zero many different ways of calculating these values also known as term weights have been developed one of the best known methods is called tf idf weighting  the definition of term depends on the application but generally terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary which is the number of distinct words occurring in the corpus  the vector space model has several disadvantages firstly long documents are represented badly because they have poor similarity values secondly search keywords must accurately match document terms and substrings of words might result in a false positive match  thirdly documents with similar context but different term vocabulary will not be associated resulting in a false negative match  finally the order in which the terms appear in the document is lost in the vector space representation  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem was names after rev thomas bayes and is a method used in probability theory this theorem aims to relate the conditional and marginal probabilities of two random events occuring and given various observations is frequently used to compute subsequent probabilities bayes theorem is also often known as bayes law  an example of where bayes theorem may be used is in the following extract  suppose there exists a school with forty percent females and sixty percent males as students the female students can only wear skirts or trousers in equal numbers whereas all the male students can only wear trousers an observer randomly sees a student from a distance and all he can see is that this student is wearing trousers what is the probability this student is female  there is a debate amongst frequentists and bayesians about how bayes theorem plays a major role around the beginnings of statistical mathematics frequentist and bayesian explanations do not agree about the ways in which probabilities should be assigned this is primarily because bayesians assign probabilities in terms of beliefs whereas frequentists assign probabilities to random events according to the frequencies of them occurring \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithm design technique used for optimisation problems such as minimising or maximising like divide and conquer dynamic programming solves problems by combining solutions to sub problems however unlike divide and conquer sub problems are not always independent as sub problems may share sub sub problems but solution to one sub problem may not affect the solutions to other sub problems of the same problem  there are four steps in dynamic programming  1 characterise structure of an optimal solution  2 define value of optimal solution recursively  3 compute optimal solution values either top down with caching or bottom up in a table  4 construct an optimal solution from computed values  an example of the type of problem for which dynamic programming may be used is given two sequences x x1 xm and y y1 yn find a common subsequence whose length is maximum  dynamic programming reduces computation by solving sub problems in a bottom up fashion and by storing solution to a sub problem the first time it is solved also looking up the solution when a sub problem is encountered again helps reduce computation however the key in dynamic programming is to determine the structure of optimal solutions  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a basic concept in object oriented programming it models the reuse of existing class code in new classes  the is a kind of relationship  for example a house is a kind of building similarly an office block is a kind of building both house and office block will inherit certain characteristics from buildings but also have their own personal characteristics  a house may have a number of occupants whereas an office block will have a number of offices however these personal characteristics don t apply to all types of buildings  in this example the building would be considered the superclass  it contains general characteristics for other objects to inherit  and the house and office block are both subclasses  they are specific types and specialise the characteristics of the superclass  java allows object inheritance when one class inherits from another class all the public variables and methods are available to the subclass  public class shape private color colour public void setcolour color newcolour  colour  newcolour  public class circle extends shape private int radius public void setradius int newradius  radius  newradius  in this example the circle class is a subclass of the shape class the shape class provides a public setcolour method which will be available to the circle class and other subclasses of shape however the private variable colour as defined in the shape class will not be available for direct manipulation by the circle class because it is not inherited the circle class specialises the shape class which means that setradius is available to the circle class and all subclasses of circle but it isn t available to the superclass shape  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank pr refers to both the concept and the google system used for ranking the importance of pages on the web the pagerank of a site refers to its importance or value on the web in relation to the rest of the sites that have been pagerank ed  the algorithm basically works like a popularity contest  if your site is linked to by popular websites then your site is considered more popular however the pr doesn t just apply to the website as a whole  different pages within a website get given different prs dependent on a number of factors inbound links backlinks  how many pages other than the ones on your website link to this particular page  outbound links forward links  how many external pages the particular page links to  dangling links  how many pages with no external links are linked to from a particular page  deep links  how many links that are not the home page are linked to from a particular page pr tries to emulate a random surfer  the algorithm includes a dampening factor which is the probability that a random surfer will get bored and go and visit a new page  by default this is 0 85 a variation on this is the intentional surfer  where the importance of a page is based on the actual visits to sites by users this method is used in the google toolbar which reports back actual site visits to google \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model is an algebraic model for representing text documents and in general any objects as vectors of identifiers such as for example index terms its first use was in the smart information retrieval system it is used in information filtering information retrieval indexing and relevancy rankings  a document is represented as a vector and each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  one of the best known schemes is tf idf weighting proposed by salton wong and yang in the classic vector space model the term specific weights in the document vectors are products of local and global parameters  relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents  the vector space model has the following limitations   search keywords must precisely match document terms word substrings might result in a false positive match   semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match   the order in which the terms appear in the document is lost in the vector space representation  long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events for example a person may be seen to have certain medical symptoms bayes theorem can then be used to compute the probability that given that observation the proposed diagnosis is the right one  bayes theorem forms a relationship between the probabilities xcof events a and b intuitively bayes theorem in this form describes the way in which one s recognition of a are updated by having observed b  p a  b  p b  a p a  p b  p a b is the conditional probability of a given b it is derived from or depends upon the specified value of b therefore it is also known as the posterior probability  p b a is the conditional probability of b given a  p a is the prior probability a it doesn t take into account any information about b so it is prior  p b is the prior or marginal probability of b and acts to normalise the probability  to derive the theorem we begin with the definition of conditional probability by combining and re arranging these two equations for a and b we get a the lemma called product rule for probabilities provided that p b is not a zero dividing both sides by p b renders us with bayes theorem \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method for solving mathematical programming problems that exhibit the properties of overlapping subproblems and optimal substructure this is a much quicker method than other more naive methods the word programming in dynamic programming relates optimization which is commonly referred to as mathematical programming richard bellman originally coined the term in the 1940s to describe a method for solving problems where one needs to find the best decisions one after another and by 1953 he refined his method to the current modern meaning  optimal substructure means that by splitting the programming into optimal solutions of subproblems these can then be used to find the optimal solutions of the overall problem one example is the computing of the shortest path to a goal from a vertex in a graph first compute the shortest path to the goal from all adjacent vertices then using this the best overall path can be found thereby demonstrating the dynamic programming principle this general three step process can be used to solve a problem  1 break up the problem different smaller subproblems  2 recursively use this three step process to compute the optimal path in the subproblem  3 construct an optimal solution using the computed optimal subproblems for the original problem  this process continues recursively working over the subproblems by dividing them into sub subproblems and so forth until a simple case is reached one that is easily solvable  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is where a new class is formed using classes which have allready been defined these classes have have some of the behavior and attributes which where existent in the classes that it inherited from the peropos of inheritance in object oriented programming is to minimize the reuse of existing code without modification  inheritance allowes classes to be categorized similer to the way humans catagorize it also provides a way to generalize du to the is a relationship between classes for example a cow is a generalization of animal similarly so are pigs  cheaters  defeining classes in this way allows us to define attributes and behaviours which are commen to all animals in one class so cheaters would natuarly inheart properities commen to all animals  the advantage of inheritance is that classes which would otherwise have alot of similar code  can instead shair the same code thus reducing the complexity of the program inheritance therefore can also be refered to as polymorphism which is where many pieces of code are controled by shared control code  inheritance can be accomplished by overriding methods in its ancestor or by adding new methods  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "there are many attributes which infulance the ranking of a page in google the main too are the content key words and links the content of a webpage generaly gives a good idea about what the page is about however there are some flaws in this for example for along time ibm web page didnt contain the word computer dispite it being strongly associated with them to solve this problem web pages can assign itself key words which contribute to its ranking in searches  the second method is the use of links the more sights which links to your web page and the higher the rank of those sights the higher the rank of your site will be this method is used as links are seen as an adoursment of a sight  with both these methods of ranking web pages there are issues key words can be compromised by sparming google solves this problem by penolizing such activity useing links to rank a page also has its problems for example link farms which have recursive links for the sole perpos of raising there ranking google takels this by useing a dampaning algorthem  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is where each document is viewed as a bag of words where there order has little significance each document is a vector where each word is a dimension the vector is then constucted of the frequency of eacher word dimension  the draw back to this approach is that the length of the document as an inpact on the vector to compensate for this you can comput the cosine similarity between your two comparism documents this will find the difference between the two vectors the dot product  ignoreing the size of them  inorder to query the search space the query can also be represented as a vector then you find the document whos vector has the greatest cosine similarities to your query there are a number of wighting sceems which can be incoperated inorder to increase the accuracy of the vextors  there are some drawbacks with this approach computing the cosine similarities between each vector can be expensive as the number of dimensions can be in the thousands to tackle this problem you can use inverted indexs and then a series heuristics inorder to inprove on this to top \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is used to compute posterior probabilities given observations for example a person may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned to each other bayesians describe probabilities in terms of beliefs and degrees of uncertainty while frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole the articles on bayesian probability and frequentist probability discuss these debates in detail \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science dynamic programming is a way of solving problems consist of overlapping subproblems and optimal substructure the method is more effiecent than naive methods  the term was first coined in the 1940s by richard bellman to describe the process of solving problems where you need to find the best decisions consecutavly in 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman equation is a central result of dynamic programming which restates an optimization problem in recursive form  dynamic programming has little connection to computer programming at all and instead comes from the term mathematical programming a synonym for optimization thus the program is the best plan for action that is produced for instance a events schedule at an exhibition is sometimes called a program programming means finding a plan of action  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance in object oriented programming is a way to form new classes using classes that have already been defined the new classes known as derived classes inherit attributes and behaviour of the existing classes which are referred to as base classes with little or no modification it is intended to help reuse existing code it is typically accomplished either by overriding one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor inheritance is also sometimes called generalization because there is a relationships represent a hierarchy between classes of objects a fruit  for instance is a generalization of orange  mango  apples and many others one can consider fruit to be an abstraction of apple orange etc since apples are fruit i e  an apple is a fruit  conversely apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank algorithm is patented by stanford university it is a link analysis algorithm employed by the google internet search engine that assigns a value used to measure the importance to each element of a hyperlinked set of documents such as the www with the purpose of  measuring its relative significance within the set  google owns exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in return for use of the patent  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "an algebraic model for representing text documents and any objects in general is known by the name vector space model it represents these as vectors of identifiers index terms are one illustration of these the vector space model was first used in the smart information retrieval system and it is utilised variously in indexing information filtering indexing and information retrieval  a document has representation as a vector every dimension is precisely related to a separate term the way in which term is defined depends entirely on the application typically terms are either single words keywords or longer phrases the dimensionality of the vector is the number of words in the vocabulary if it is the words that are chose to be the terms so the same rule applies with keywords and indeed longer phrases  if a term occurs in the document its value in the vector is non zero several different ways of computing these values additionally known as term weights have been developed one of the most famous schemes is tf idf weighting  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "baye s theorm in connection with conditional probabilities is of fundamental importance since it permits a calculation of prob ab from prob ba  statistical information that is often gathered in great volume can therefore be avoided \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is an extremely powerful general tool for solving optimization difficulties on left right ordered item for example character strings it is similar to divide and conquer however is differentiated as its subproblems are not independent it is easily applicable in relative terms once understood however until one has witnessed enough examples it looks like magic  dp minimizes computation by solving subproblems from the base upwards storing solution to a subproblem when it is initially conquered and looking up the solution when the subproblem is experienced for a second time  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula the new classes known as derived classes take over or inherit attribute and behaviour of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references pagerank uses in google toolbar measures popularity of a site marketing value updated periodically in google directory pagerank sort links within categories volunteers evaluate classify annotate open directory project using pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the representation of a set of documents as vectors in a common vector space is known as the vector space vector space model and is fundamental to a host of information retrieval ir operations including scoring documents on a query document classification and document clustering we first develop the basic ideas underlying vector space scoring a pivotal step in this development is the view of queries as vectors in the same vector space as the document collection  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is an important theorem relating conditional probabilities it allows us to calculate prob a b from prob b a  bayes theorem is important because it can save us from gathering vast amounts of statistical evidence the main theory is prob a b  prob b a  prob a  prob b  it means using prob win rain from earlier we can find the probability that it rained on a day that harry won a race \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming objects are grouped together into classes according to their type structure and the functions that can be performed on them inheritance is a process in object oriented programming in which objects acquire or inherit the properties of objects of another class it is therefore used to create relationships between one object and another each class groups together objects of a similar type with similar properties new classes can be formed by this process whose objects will have properties of both the classes from which this new class is formed a superclass has all of the properties of the subclasses below it at the same time subclasses are each distinctive from each other but related via the superclass subclasses are said to extend superclasses due to these relationships object oriented programmes tend to be easier to modify since they do not need to be changed when a new object with different properties is added instead a new object is made to inherit properties of objects which already exist inheritance can be divided into two main processes single inheritance and multiple inheritance single inheritance means that the class can only inherit from one other class whereas multiple inheritance allows for inheritance from several classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm is used to designate every aspect of a set of hyperlinked documents with a numerical weighting it is used by the google search engine to estimate the relative importance of a web page according to this weighting the system uses probability distribution to determine the odds that a person randomly clicking on links will arrive at any given page following this each web page is given a ranking of 0 10 according to its relevance to a search the pagerank is calculated by taking into consideration the number of inbound links and the pagerank of the pages supplying these links this means therefore that if a webpage is linked to others that have a high ranking then it too will receive a high rank  due to the nature of the pagerank system it is susceptible to manipulation and has been exploited so that certain pages are given a false exaggerated ranking in these cases only goggle has access to the genuine pagerank however much research has been conducted into methods of avoiding links from documents with a false pagerank to try and iron out the bugs in this system and from 2007 google has actively penalized schemes which try to increase rankings artificially \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model is an algebraic model used to represent text documents and any objects generally as vectors of identifiers for instance index terms its applications include information filtering information retrieval indexing and relevancy rankings with reference to this model documents are represented as vectors each dimension corresponds to a separate term the value of a vector is non zero if a term occurs in the document several different ways have been developed of calculating these values also known as term weights  one of the best known schemes is tf idf term frequency inverse document frequency weighting  the model can be used to determine the relevancy rankings of documents in a keyword search using the assumptions of document similarities theory by comparing the original query vector where the query is represented as same kind of vector as the documents and the deviation of angles between each document vector  the classic vector space model was put forward by salton wong and yang and is known as term frequency inverse document frequency model in this classic model the term specific weights in the document vectors are products of local and global parameters in a simpler term count model the term specific weights are just the counts of term occurrences and therefore do not include the global parameter  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events and is named after the reverend thomas bayes 1702 1761  who studied how to compute a distribution for the parameter of a binomial distribution it is valid in all common interpretations of probability it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty applications of bayes theorem often assume the philosophy underlying bayesian probability that uncertainty and degrees of belief can be measured as probabilities one of bayes results proposition 5 gives a simple description of conditional probability and shows that it can be expressed independently of the order in which things occur if there be two subsequent events the probability of the second b n and the probability of both together p n and it being first discovered that the second event has also happened from hence i guess that the first event has also happened the probability i am right i e  the conditional probability of the first event being true given that the second has also happened is p b  note that the expression says nothing about the order in which the events occurred it measures correlation not causation \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is an algorithmic technique used to solve certain optimization problems where the object is to find the best solution from a number of possibilities it uses a so called bottom up approach meaning that the problem is solved as a set of sub problems which in turn are made up of sub sub problems sub problems are then selected and used to solve the overall problem these sub problems are only solved once and the solutions are saved so that they will not need to be recalculated again whilst calculated individually they may also overlap when any sub problem is met again it can be found and re used to solve another problem since it searches all possibilities it is also very accurate this method is far more efficient than recalculating and therefore considerably reduces computation it is widely used in computer science and can be applied for example to compress data in high density bar codes  dynamic programming is most effective and therefore most often used on objects that are ordered from left to right and whose order cannot be rearranged this means it works well on character chains for example  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is one of the basic concepts of object oriented programming  its objective is to add more detail to pre existing classes whilst still allowing the methods and variables of these classes to be reused  the easiest way to look at inheritance is as an is a kind of relationship  for example a guitar is a kind of string instrument electric acoustic and steel stringed are all types of guitar the further down an inheritance tree you get the more specific the classes become  an example here would be books  books generally fall into two categories fiction and non fiction  each of these can then be sub divided into more groups  fiction for example can be split into fantasy horror romance and many more  non fiction splits the same way into other topics such as history geography cooking etc  history of course can be sub divided into time periods like the romans the elizabethans the world wars and so on \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "a websites page rank is how important it is on the web  it is essentially a popularity meter  popularity or importance is determined by the amount of links relating to the page there are there are four different types  inbound links from other pages to yours  outbound links from your page to others  dangling links to a page which has no links to others  deep links to a specific page usually bypassing the homepage  the page rank algorithm takes the probability of a random surfer becoming bored and requesting another random page otherwise known as the dampening factor away from 1 and divides this number by the number of pages in the system adding it to the dampening factor multiplied by the page rank of a linked page divided by the number of outbound links on that linked page  adding on this last section for every other page linked to from the original page  google uses this algorithm to assist intentional surfers in finding the best websites to suit their needs  one of the problems with this popularity algorithm is that it is easily manipulated and can give false values hence the frequent recalculating of page ranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the algebraic model for representing text documents and objects as vectors of identifiers is called the vector space model  it is used in information filtering indexing relevancy rankings and information retrieval it was first used in the smart information retrieval system  when a document is represented as a vector each dimension corresponds to a separate term a term which occurs in the document has a value in the vector of non zero other ways of computing these values or weights have been developed the most popular is tf idf weighting depending on the application the definition of term varies  single words keywords and occasionally longer phrases are used for terms  the dimensionality of the vector if words are used as terms is the total number of words available for use  by using the assumptions of the document similarities theory the relevancy rankings of documents in a keyword search can be worked out by comparing the deviation of angles between vectors both within the document and the original query where the vectors of both are the same type the limitations of the vector space model are thus  due to poor similarity values long documents are poorly represented  false positive matches may be returned if search keywords do not precisely match document terms  false negative matches could be returned when documents share a context but have different term vocabulary  vector space representation results in the loss of the order which the terms are in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem relates the conditional and marginal probabilities of two random events  it is mainly used to calculate the probability of one events outcome given that a previous event happened  for example the probability that a doctors diagnosis is correct given that the doctor had previously observed symptoms in the patient  bayes theorem can be used for all forms of probability however it is currently at the centre of a debate concerning the ways in which probabilities should be assigned in applications  the theorem states that the probability of event a happening given event b is the probability of b given a multiplied by the probability of a regardless of b all divided by the probability of b regardless of a which acts as a normalising constant  bayes theorem formed in this way basically details how ones beliefs about event a are renewed or updated knowing that event b happened  when calculating conditional probabilities such as these it is often useful to create a table containing the number of occurrences or relative frequencies of each outcome for each of the variables independently  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping sub problems and optimal substructure the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm dynamic programming usually takes one of two approaches the top down approach the problem is broken into sub problems and these sub problems are solved and the solutions remembered in case they need to be solved again this is recursion and memorization combined together and the bottom up approach all sub problems that might be needed are solved in advance and then used to build up solutions to larger problems this approach is slightly better in stack space and number of function calls but it is sometimes not intuitive to figure out all the sub problems needed for solving the given problem some programming languages can automatically memorize the result of a function call with a particular set of arguments in order to speed up call by name some languages make it possible portably e g scheme common lisp or perl  some need special extensions this is only possible for a referentially transparent function  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a method of forming new classes using predefined classes the new classes are called derived classes and they inherit the behaviours and attributes of the base classes it was intended to allow existing code to be used again with minimal or no alteration it also offers support for representation by categorization in computer languages this is a powerful mechanism of information processing vital to human learning by means of generalization and cognitive economy inheritance is occasionally referred to as generalization due to the fact that is a relationships represent a hierarchy between classes of objects inheritance has the advantage of reducing the complexity of a program since modules with very similar interfaces can share lots of code due to this inheritance has another view called polymorphism where many sections of code are being controlled by some shared control code inheritance is normally achieved by overriding one or more methods exposed by ancestor or by creating new methods on top of those exposed by an ancestor inheritance has a variety of uses each different use focuses on different properties for example the external behaviour of objects internal structure of an object inheritance hierarchy structure or software engineering properties of inheritance occasionally it is advantageous to differentiate between these uses as it is not necessarily noticeable from context  \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page pagerank can be calculated for collections of documents of any size it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank the pagerank theory holds that even an imaginary surfer who is randomly clicking on links will eventually stop clicking the probability at any step that the person will continue is a damping factor d various studies have tested different damping factors but it is generally assumed that the damping factor will be set around 0 85  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "within information retrieval each document in a set can be represented as a point in high dimensional vector space this representation is called the vector space model information retrieval queries are also represented as vectors in the same vector space these are then used in conjunction with the document vectors to find relevant documents the two vectors are compared and the documents with a higher document query similarity are ranked higher in terms of relevance there are a variety of techniques that can be used to compare the two vectors the most frequently used method for the vector space model is the cosine coefficient which calculates the angle between the two vectors and produces a value between 0 and 1  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a mathematical formula used to calculate conditional probabilities given the probability of event a given event b bayes theorem can be used to calculate the probability of b given a  this is achieved using the conditional probability of b given a and the prior probabilities of both events a and b for example suppose there is a bag of coloured balls with 25 red ones and 75 black ones lucky joe likes to predict the colour of the ball he selects and he is 80 accurate joe records all of his results and about 0 5 of the time he accidently records the wrong results using all of this information more probabilities can be inferred including using bayes theorem to calculate various probabilities like joe recording correctly if he guesses correctly or joe recording incorrectly when his guess was correct and other like combinations  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a faster method of solving problems that make use of optimal substructure overlapping sub problems and memoization it has no relationship to computer programming instead it is a process of finding a satisfactory algorithm  optimal substructure is the process of using the optional solutions to sub problems to find the optimal solution to the overall problem when the same sub problem solutions can be used to solve various bigger problems it is said to have overlapping sub problems memoization is used in order to save time the solutions are stored rather than be recomputed a solution can be disposed of once we are positive that it will no longer be required in some cases a solution to a future problem can be computed in advance  there are two main approaches for dynamic programming the first is the bottom up approach although it is not always simple to find all of them any required sub problems are solved in advance and then used to create solutions to larger problems the other method is the top down approach which is a method that combines memorization and recursion the main problem is divided into sub problems which are solved and stored for future use \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance allows programs developed in an object orientated language to reuse code without having it replicated unnecessarily elsewhere within the program  to achieve this the programmer has to note generalisations and similarities about various aspects of the program  for example a program could exist to model different forms of transport at first glance a car and a train may not have much in common but abstractly both will have a speed at which they are travelling a direction and a current position methods utilising this data can be specified high up in the inheritance hierarchy for example in a transport class for example you could have a method which works out the new position of a train after travelling x minutes in direction y likewise you might want to be able to find out the same information for an object of the type car  inheritance means that if such a method was defined in the superclass of the train and car classes any car or train object can utilise it  the train and car subclasses are said to extend the transport class as they will have additional characteristics which they dont share e g passenger capacity would be a class variable of both car and train but have different values  and a train may have methods along the lines of is toilet engaged if you then wanted to add additional forms of transport such as an aeroplane you may wish for that also to have a toilet engaged function then you could have an extended hierarchy where a mass transport class extends the transport class under which youd have a train and aeroplane which would inherit characteristics from both super classes \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the algorithm that google uses to assign a weighting to each element of a linked set of documents with the purpose of measuring its relative importance within the set  a particular websites pagerank results from a vote from other pages on the internet about how important that website actually is a link to a page is seen as a vote of support the pagerank depends on the pagerank rating and number of all pages that have links to it additionally if a page is linked to by pages with a high pagerank rating this increases the rating of the original page the pagerank scale ranges from 0 10 the rating of a certain page is generally based upon the quantity of inbound links as well as the perceived quality of the pages providing the links  pagerank could be described as a probability distribution representing the chance that someone randomly clicking on links will reach a certain page the pagerank calculations require iterations through the collection of web pages to alter approximate pagerank values to accurately reflect the actual rank in order to prevent spamming google releases little information on the way in which a pagerank is calculated the pagerank algorithm has led to many sites being spammed with links in an attempt to artificially inflate the pagerank of the linked page notably in blog comments and message boards in 2005 a nofollow tag was added as an attribute of a html link to be used where google shouldnt change the pagerank of the linked page as a result of the link \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model or term vector model is an algebraic way of representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings its first application was in the smart information retrieval system a document can be represented as a vector every dimension relates to a different term if a term appears in the document the terms value in the vector is non zero many different methods of calculating these values sometimes known as term weights have been developed tf idf weighting is one of the most well known schemes  see below example  the definition of a term depends on the application normally a term is a single word keyword or a longer phrase if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has some limitations 1 longer documents are represented poorly because the documents have poor similarity values namely a small scalar product and a large dimensionality 2 search keywords have to precisely match document terms word substrings could potentially result in a false positive match 3 semantic sensitivity documents with a similar context but different term vocabulary won t be associated resulting in a false negative match  4 the order in which terms appear in the document is lost in a vector space representation \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  suppose there is a co ed school having 60 boys and 40 girls as students the girl students wear trousers or skirts in equal numbers the boys all wear trousers an observer sees a random student from a distance all they can see is that this student is wearing trousers what is the probability this student is a girl the correct answer can be computed using bayes theorem the event a is that the student observed is a girl and the event b is that the student observed is wearing trousers to compute p a b  we first need to know p b a  or the probability of the student wearing trousers given that the student is a boy this is given as 1 p a  or the probability that the student is a girl regardless of any other information since the observers sees a random student meaning that all students have the same probability of being observed and the fraction of girls among the students is 40  this probability equals 0 4 p a  or the probability that the student is a boy regardless of any other information a is the complementary event to a  this is 60  or 0 6 p b a  or the probability of the student wearing trousers given that the student is a girl as they are as likely to wear skirts as trousers this is 0 5\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a very powerful mathematical technique often utilised in programming for solving optimization problems normally minimizing or maximizing  greedy algorithms focus on making the best local choice at each decision making stage without a proof of correctness such an algorithm is likely to fail with dynamic programming we can design our own algorithm which searches for all possibilities which ensures correctness whilst storing the results to avoid having to recomputed leading to computational efficiency  dynamic programming solves problems by combining the solutions of subproblems these subproblems are not however independent subproblems can share subsubproblems but the solution to one subproblem doesn t necessarily affect the solutions to other subproblems stemming from the same problem dynamic programming reduces computation time by solving subproblems in a bottom up way it stores the solution to a subproblem the first time it is solved meaning that it can look up the solution when that subproblem is encountered subsequently  the key to dynamic programming is to find the structure of optimal solutions the steps required are as follows 1 generalise the structure of an optimal solution 2 recursively define the value of an optimal solution 3 compute the optimal solution values either top down with caching  or bottom up using a table 4 generate the optimal solution of these computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      " inheritance is an important feature in object orientated programming this is because it allows new classes to be made that extend previous classes and to go into more detail  this is carried out by allowing the new class to reuse the existing class methods and variables whilst also creating class specific methods and variables this means that the new class the subclass is a more specialised version of the original or superclass  because of this it means that the subclass can use all the public methods and variables from the superclass however any private methods or variables are still private  also it should be noted that a class can only extend one class e g can only be a subclass to one superclass however a superclass can have more then one subclass and a class can both be a subclass and a superclass if this occurs then all of the non private methods and variables can be used by the most specialised class  this means that inheritance is used when types have common factors and these would be put into the superclass then the subclass es then extend these to add more detail an example of this could be using a superclass of employee and then to have two subclasses called fulltime and part time as employee could have name address and other details whilst full time could just have salary and part time could work out the salary from part time hours worked as the full time members of staff wouldnt need these \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the first thing to consider when talking about googles pagerank algorithm is that a pagerank is essentially how important that web page is to the internet so in essence it is a popularity contest between webpages  originally search engines used highest keyword density however this could be abused if keyword spamming was implemented instead google uses a system that is based on sites linking to each other e g the more important a site is that is linked to yours the higher your site will become  the algorithm google actually users is based on 4 factors total number of pages dampening factor pagerank of a single page and the number of outbound links a dampening factor is used to counter random surfers who get bored and then switch to other pages this formula is then re used until the results seem to converge together to find the pagerank so it is calculated iteratively  pagerank is used by google to measure a popularity of the site and a number between 0 10 is assigned to each webpage depending on their pagerank this allows google to calculate a marketing value for different webpages  also it should be noted that the pagerank is periodically updated every 3 to 6 months this is counter hackers influence on different pageranks \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "a vector space model is an algebraic model for representing text documents as vectors of identifiers a possible use for a vector space model is for retrieval and filtering of information other possible uses for vector space models are indexing and also to rank the relevancy of differing documents to explain further vector space models basically a document is characterized by a vector with each separate term corresponding to the differing dimensions there has been multiple ways of trying to compute the different possible values for vector space models with the most recognised being the tf idf weighting the differing application has a direct influence on what the definition of the term means a normal term is usually a single word keywords or longer phrases the number of unique words in the vocabulary denotes the dimensionality if words are used for the terms however whilst vector space modelling is useful there are 4 key problems with using it they are that the order of the terms are lost keywords must be precise if searched for bigger documents have a poor similarity value due to being poorly represented and two documents based on the same topic wont be associated if term vocabulary differs \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " in probability theory bayes theorem also called bayes law after rev thomas bayes compares the conditional and marginal probabilities of two random events it is often used to calculate posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to calculate the likelihood that a proposed analysis is accurate given that observation  as an official theorem bayes theorem is valid in all universal interpretations of probability however it plays a fundamental role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem compares the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name p a is the previous probability of a it is previous in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the subsequent probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the previous \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure the method takes much less time than naive methods the term was originally used in the 1940s to describe the process of solving problems where one needs to find the best decisions one after another  the field was founded as a systems analysis and engineering topic that is recognized by the ieee the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  in defining this inheritance hierarchy we have already defined certain restrictions not all of which are desirable singleness using single inheritance a subclass can inherit from only one superclass visibility whenever client code has access to an object it generally has access to all the object s superclass data static the inheritance hierarchy of an object is fixed at instantiation when the object s type is selected and does not change with time \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank is a recursive algorithm used by google to determine which webpages are more important than others the algorithm considers the importance of a webpage to be reflected by how many other webpages link to that page and the importance of those pages  for each page that links to a page a the pagerank between zero and one is calculated iteratively according to the following two key factors the probability of a user navigating away from a page randomly the pagerank of any page that links to a divided by the total number of outbound links from that page this assumes that a link among many outbound links is less valuable than a link among fewer outbound links a variation of the pagerank method bases the importance of a webpage on how many visits the page gets  the method can be abused when people deliberately link to sites in order to raise a site s pagerank however it is still a good indicator for search engines to use as a variable in deciding on the most appropriate results according to a query  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in the vector space model vsm  documents take the form of bags of words  a standard information retrieval approach which represents documents as in a mathematical bag structure recording what terms are present and how often they occur  the vector space model is used in information retrieval to determine how similar documents are to one another and how similar documents are to a search query  in a collection of documents each document can be viewed as a vector of n values the terms in the document  where each term is an axis queries can also be represented as vectors on this vector space model and so deciding which document matches the query the closest becomes a matter of selecting the document vector which is nearest to the query vector  the query vector is compared to each document vector in turn using a vector similarity measure  which is the cosine of the angle between the query vector and the document vector  this equation is calculated by dividing the dot product of the query vector and the document vector by the modulus of the query vector multiplied by the modulus of the document vector the denominator takes into account differences in the length of the vector and has the effect of normalising the length whichever document returns the highest cosine similarity score is considered to be the closest matching document to the query  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations  bayes theorem is expressed mathematically as  p a b p b a p a  p b  where p a b is the conditional probability of a given b p a is the prior probability of a p b is the prior probability of b and p b a is the conditional probability of b given a  bayes theorem relates the conditional and marginal probabilities of two random events p a and p b  and is valid in all common interpretations of probability for example in a school in made up of 3 5 boys and 2 5 girls the girls wear skirts of trousers in equal numbers and the boys all wear trousers if a student is observed from a distance wearing trousers bayes theorem can be used to determine the probability of this student being a girl  p a is the probability of the student being a girl which is 2 5  p b a is the probability of the student wearing trousers given that the student is a girl which is 0 5 p b is the probability of a random student wearing trousers which can be calculated as p b  p b a p a  p b a p a  where  denotes a complementary event which is 0 8  therefore the probability using the formula is 0 25  bayes theorem is often used to compute posterior probabilities given observations for instance the probability that a proposed medical diagnosis is correct given certain observed symptoms  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a problem solving method which solves recursive problems the term is derived from mathematical programming which is commonly referred to as optimisation hence dynamic programming is an optimal method of solving the problems and takes much less time than na ve methods  dynamic programming uses the properties of optimal substructure overlapping subproblems and memoization to create an algorithm to solve such problems optimal substructure means that the structure of the problem is made up of sub problems which can be used to find the solution to the problem overall a problem with overlapping subproblems means that the same subproblems may be used to solve many different larger problems each sub problem is solved by being divided into sub subproblems until a case is reached which is solvable in constant time memoization stores solutions which have already been computed in order to reduce unnecessary re computation  dynamic programming can be divided into two main approaches top down and bottom up the top down approach breaks the problem into subproblems which are solved and remembered using a combination of memoization and recursion the bottom up approach solves all subproblems that might be need in advance and then uses these solutions to build up the solutions to the bigger problem  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "when we talk about inheritance in object oriented programming languages which is a concept that was invented in 1967 for simula we are usually talking about a way to form new classes and classes are instances of which are called objects and involve using classes that have already been defined  derived classes are intended to help reuse existing code with little or no modification and are the new classes that take over or inherit attributes and behavior of the pre existing classes usually referred to as base classes or ancestor classes  categorization in computer languages is a powerful way number of processing information and inheritance provides the support for representation by categorization furthermore  it is fundamental for helping humans learn by means of generalization in what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive processing which involves less information being acquired to be stored about each specific entity but in actual fact only its particularities an instance of a fruit is a generalization of apple  orange  mango and many others inheritance can also sometimes be referred to as generalization because is a relationships represent a hierarchy amongst classes of objects it can be considered that fruit is an abstraction of apple orange etc conversely since apples are fruit they may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  modules with sufficiently similarities in interfaces would be able to share a lot of code and therefore reducing the complexity of the program this can be known as one of the advantages of inheritance therefore inheritance can be known to have a further view a dual which describes many parts of code that are under control of shared control code named as polymorphism  on the other hand inheritance is normally accomplished either by replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor a well known term used for this replacing act is called overriding \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a link analysis algorithm that is used by search engine such as google internet that assigns a numerical weighting to every element of a hyperlinked set of documents like the world wide web with the hope of measuring the relative importance held in the set the algorithm may be applied to any numbr of entities with reciprocal quotations and references the weight taking a numerical value which assigns to any given element e is also known as the pagerank of e and is denoted by pr e  a trademark of google has the name pagerank  and this process has been patented u s patent 6 285 999  nevertheless the patent is assigned to the university of stanford and not to google google has exclusive license rights on the patent from the university of stanford and the university received 1 8 million shares in google in exchange for use of the patent the in the year 2005 shares were sold for 336 million \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "nformation retrieval ir is the science of searching for documents for information within documents and for metadata about documents as well as that of searching relational databases and the world wide web ir is interdisciplinary based on computer science mathematics library science information science information architecture cognitive psychology linguistics statistics and physics there is overlap in the usage of the terms data retrieval document retrieval information retrieval and text retrieval but each also has its own body of literature theory praxis and technologies  automated information retrieval systems are used to reduce what has been called information overload  many universities and public libraries use ir systems to provide access to books journals and other documents  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "the probability of an event happening mean considering the likelihood of or the number of the instance occurring and dividing this value by the total number of events the equation for this calculation would look as follows probability p  number of instance  total number of events on the other hand probability theory p usually involves assigning values to events for example  p 1 event is certain to occur p 0 event is certain not to occur p 0 5 event occurs half of the time  there is also conditional probability which is usually interested in the way variables relate to each other bayes theorem is the name given to an important theorem relating conditional probabilities and it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming dp is in basic terms an algorithm design technique that is used for optimization problems and often involves minimizing or maximizing  furthermore by combining solutions to subproblems dp solves problems subproblems may include and contain many other subsubproblems and even in such cases the solution to one subproblem may not affect the solutions to other subproblems involved in the same problem  by solving subproblems in a bottom up fashion which is basically when storing solution to a subproblem the first time it is solved and looking up to find the solution when a subproblem is come across once more  this would cause dp to reduce computations  the following is a generalization path to be taken in dynamic programming firstly it is needed to characterize the structure of an optimal solution secondly to define the value of the optimal solution recursively furthermore to compute the optimal solution values either by following a top down method with caching or a bottom up method in a table the last point would be to construct an optimal solution from the computed values \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is the ability to specify one class to be a subclass of another this leads to a hierarchy of classes with the child classes inheriting and specialising  and sometimes adding to  the functionality and data structures of the parent classes the hierarchy that is formed is also useful for the organisation of classes and objects as it defines a relationship between the child and the parent the child class is a kind of the parent class  inheritance is useful for situations where several classes share common features such as needed functions or data variables in addition to this child classes can be referenced in terms of their parent classes which can be useful when storing large data structures of objects of several classes which can all be referenced as one base class inheritance is a core aspect of object oriented programming and is available in some form or another in most if not all object oriented languages available today most of these languages provide an extend keyword which is used to subclass another also the functions and data variables that are inherited by the subclasses can be controlled through the use of visibility modifiers \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the google search engine uses a link analysis algorithm called pagerank to assign a relative numerical importance to a set of hyperlinked documents such as the world wide web for a given page it s importance the pagerank value results from a ballot among all the other pages in the set for a page to give a vote to another it must link to it and so the pagerank depends on the number of incoming links anf the pagerank of those pages that provide the links pages that are linked to by many high ranking pages will themselves obtain a high rank if a page has no incoming links there is no support for that page the pagerank is a numeric weighting of 0 to 10 and denotes how important a site is in google s eyes like the richter scale the pagerank is a value on a logerithmic scale that is derived from a probability in addition to the quantity and quality of inbound links other factors affect the pagerank such as the number of visits to the page and the search words that are used on the page to prevent sites from manipulating or spoofing pagerank very little details are provided by google as to what factors actually affect it \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "vector space model or term vector model as it is also known is an algebraic model for representing objects although it is mainly used for text documents as vectors of identifiers for example index terms it is used in information retrieval and filtering indexing and relevancy rankings and was first used in the smart information retrieval system  a document is represented as a vector with each dimension corresponding to a separate term if a term occurs in the document the value will be non zero in the vector many different ways of computing these values aka term weights have been developed one of the best known schemes is tf idf weighting  the way that a term is defined depends on the application typically terms are single words keywords or sometimes even longer phrases if the words are chosen as the terms the number of dimensions in the vector is the number of distinct words in the corpus  relevancy ranks for documents in a keyword search can be calculated this uses the assumptions of document similarities theory by comparing the difference of angles between each document vector and the original query vector where the query is represented as same format vector as the documents  generally it is easier to calculate the cosine of the angle between the vectors instead of the angle itself a zero value for the cosine indicates that the query and document vector are orthogonal and so had no match this means the query term did not exist in the document being considered  however the vector space model has limitations long documents are poorly represented due to their poor similarity values a small scalar product and a large dimensionality  search keywords must match precisely the document terms word substrings might result in a false positive match  similar context documents but different term vocabulary won t be associated leading to a false negative match  and the order that the terms appear in the document is not represented in the vector space model \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms  bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b p b  a x p a p b  each term in bayes theorem has a conventional name p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in the field of computer science term dynamic programming relates to the style of programming that breaks a large problem down into smaller subproblems and generally allows for the finding of the optimal solution when the problem is split into subproblems these themselves may be split into smaller problems and so on until they cannot be reduced any more it is also common for dynamic programming to make use of recursion and the saving of previous results for faster computation later this also leads to higher efficiency as calculations are not being redone for example when a problem is reduced into sub problems and those are then reduced further it may be that there are common subsubproblems and so only one calculation needs to be done and the result saved to help solve more than one subproblem an example of this gain in efficiency is a path finding problem if there are two distinct routes in a network of 10 nodes tagged a to j then if the two routes share a common section say between nodes b and d  the cost of that section should be calculated for the first route and saved then when the second route is being processed the cost of b to d does not need to be calculated again in general dynamic programming is used on optimisation problems where the most efficient solution is needed areas where this sort of programming is useful is in ai computer graphics compression routines and biomedical applications \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is a concept in object oriented programming where a child or sub class inherits characteristics from a parent or super class  the concept takes its name from genetic inheritance where a child can inherit genetic characteristics from its parents  inheritance at its simplest allows programmers to model a relationship where one object is a kind of another  for instance two classes one representing an undergraduate student and another representing a post graduate student could both be said to belong to a more generalised class representing all students  similarly we could say that dogs and cats are two kinds of animal or that bridges and skyscrapers are two types of man made structure  subclasses are said to extend or specialise their superclasses  attributes variables and behaviours functions that are common between classes can be included in the definition of the superclass leaving the subclass definitions containing only the attributes and behaviours that are unique to that class  inheritance can be used to create a multiple level architecture of classes  in such an architecture even the bottom most subclasses inherit all of the attributes and behaviours that are defined in the very top most superclasses  this can save the programmer time because it renders unnecessary a lot of code duplication \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is an algorithm that was developed by google to provide the most relevant search results to its users queries  pagerank along with similar algorithms developed by google s competitors for their search engines is part of the second generation of technologies designed to rate the importance of web pages the first which was solely based on keywords in the page content and meta data could easily be influenced by those wishing to obtain a higher ranking for their less relevant pages  the different with pagerank is that it tries to determine a web page s relevance to users by attempting to determine its importance  it does this by assigning it a value of importance that is dependant upon the number of web sites that link to that page taking into account the importance value or pagerank of those pages  the pagerank is computed iteratively and it is found that the pagerank values converge fairly rapidly  although it is much better than simple keyword based ranking algorithms pagerank is not infallible we have an internet where advertising revenue can make up most  and quite frequently all  of a web site s income and the people that run these web sites will always be trying to trick the system into giving their pages a higher pagerank  one of google s attempts to counter this is their google toolbar browser plugin  google toolbar is a free tool which provides a number of useful functions in a convenient location the users web browser window  google s payoff is that it gets to track the behaviour of actual users  this allows them to see whether their pagerank algorithm is accurate in assigning high pagerank values to the most relevant web pages and just as importantly low values to those that are irrelevant and try to fool the system  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "there are a large number of models used in solving the problem of information retrieval and they are all based on one of three mathematical bases set theory algebra and probabilistic  the vector space model is one of these methods and it is an algebraic model  in the vector space model a document is represented as a vector  within this vector each dimension corresponds to a separate term where a term is typically a single word keyword or phrase  if the term doesn t occur within the document the value in the vector is zero  if a term occurs in the document its value is non zero  to calculate how relevant each document is in a keyword search the cosine value of the angle between the vectors is easier to calculate instead of the actual angle  the vector space model however is not without its limitations they have small similarity values long documents are poorly represented the order of words does not matter false positive matches may be brought about by terms contained within words themselves and documents that should match but use different semantics may return false negative matches  there are a number of other models that are based on or extend the vector space model and these are designed to try to eradicate these problems \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem often called bayes law connects the conditional and marginal probabilities of two arbitrary events one of its uses is calculating posterior probabilities given observations  bayes theorem plays a key role in the debate around the principles of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications  bayes theorem is useful in evaluating the result of drug tests  if a test can identify a drug user 99 of the time and can identify a non user as testing negative 99 of the time it may seem to be a relatively accurate test  however bayes theorem will reveal the flaw that despite the apparently high accuracy of the test the probability that an employee who tested positive actually did use drugs is only about 33 \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the term was originally used in the 1940s by richard bellman  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization  the program is the optimal plan for action that is produced  for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all ruit such as being a fleshy container for the seed of a plant  inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "the pagerank algorithm used by google harnesses the implicit collective intelligence present in the structure of the world wide web any page on the internet will generally link to at least one other by modelling this link structure as a graph we can build up a symbolic representation of the world wide web  as the basic level the nodes with the highest degrees can be considered the most popular and by inference the most important  which can be used to rank the pages when returning search results  expanding on this theory we can then say that the links from an important pages are themselves more important using this idea we can adjust the rankings of our pages so that pages linked to be the most important pages are considered more relevant  the actual google pagerank algorithm is much more complex than this but follows the same underlying principles it incorporates some more advanced reasoning to avoid website creators exploiting their knowledge of the algorithm to artificially increase their pagerank through use of web rings and other similar reciprocal hyperlinking schemes \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "using the vector space model for information retrieval models all pages and queries as high dimensional sparse vectors each item in the vector represents a different keyword  the similiarity betweeen two pages or a query and a page can be computed using the dot product formula to find the cosine between them this represents the angle between them but in n dimensional space results will range from 1 to 1 with 1 being a close match normally the vectors will not have any negative values so results will always be greater than or equal to 0 the cosine is computed using cos a a b  a b \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory the prior and conditional probabilities of two random events are related by bayes theorem the theorem is often used when we have observations and wish to compute posterior probabilities  for example given an observation that a patient is seen to have certain symptoms we can use bayes theorem to compute the probability that a suggested diagnosis is correct  p a is the prior probability of a p a b is the conditional probabilty of a given b p b a is the conditional probabilty of b given a p b is the prior probability of b and must be non zero bayes theorem is given by p a b p b a p a  p b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in computer science and mathematics dynamic programming is a method of problem solving that utilises the properties of overlapping subproblems and optimal substructure and thus the method takes much less time than more naive methods  in dynamic programming  the word programming has no real connection to computer programming at all it actually comes from the term mathematical programming  a synonym for optimisation thus the program is the optimal plan of action that is being produced for example a schedule of events at an exhibition is sometimes called a programme programming in this sense means finding an acceptable plan an algorithm \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "inheritance is the ability of a subclass to inherit default protected and public attributes and methods from its superclasses each object except java lang object can be cast to an object of one of its superclasses however an object cannot be cast to a class which is no relative of it here is an example of inheritance we have the class of all living things which have attributes like weight and age we have the classes of animals plants viruses and fungi that are subclasses of the class of all living things the animals have their unique attributes organs hair etc  and methods walking mating etc  they also inherit the attributes and methods of its superclass animals can be treated cast to living things however animals cannot be treated as fungi in object oriented programming inheritance is also dependant on access level modifiers for example private attributes and methods cannot be inherited virtual attributes and methods can be shadowed overridden in java all attributes and methods are implicitly virtual object variable can store a reference to the same class or a subclass i e this or more specialised version  however object variables cannot store references to a superclass i e less specialised version of the original class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "page rank algorithm is used to determine a webpages importance or relevance in the web dependant on certain criteria the criteria may include numbers of word matches with the search terms number of other webpages that link this one and or cite it as a source number of unique visits for certain amount of time etc there are some techniques that try to fool the search engines like link farms keyword spamming and a lot of meta tags the last two are somewhat easier to be dealt with simply by being ignored most of the time  link farms are groups of sites that are producing links between each other pursuing higher link counts the reason for such manipulations is the pursuit of higher page rank so even higher number of users will see the page which will lead to higher income link farms can be exploited by joining to them and get inbound linkage but refuse to add links for ones own site to the sites from the link farm googles toolbar tries to follow the intentional user model by counting the visits from actual users i e not computer bots to a website page ranks can be calculated either recursively or iteratively one of the most important uses of page rank is its meaning to advertising \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as index terms it is used in information filtering information retrieval indexing and relevancy rankings it was used in the first time in the smart information retrieval system  a document is represented as a vector each and every dimension corresponds to a separate term if a term exists in a document its value in the vector is not equal to zero a couple of different algorithms of computing these values also known as term weights have been created one of the most popular schemes is tf idf weighting  the definition of term is dependent on the application typically terms are keywords single words or longer phrases provided that words are selected to be the terms the dimensionality of the vector is equal to the number of words in the vocabulary  it is easiest to calculate the cosinus of the angle between the vectors instead of the angle by the formula  cos theta v1 v2  v1  v2 a null cosinus value says that the query and document vector were orthogonal and had no match which means that no term of the query was ever encountered in the document \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem or bayes law after rev thomas bayes provides relation between the conditional and marginal probabilities of two random events it is usually used to calculate posterior probabilities given observations for example a patient might be observed to show certain symptoms bayes theorem could be used to compute the probability that a certain diagnosis is right given that observation  since it is a formal theorem bayes theorem holds in all popular interpretations of probability  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability  p a b  p a b p a p b  terms in bayes theorem are named by a convention  p a is the prior probability or marginal probability of a it does not take into account any information about b and therefore is considered prior p a b is the conditional probability of a given b it it is derived from or depends upon the specified value of b usually it is called the posterior probability p b a is the conditional probability of b given a p b  a k a the normalizing constant is the prior or marginal probability of b  obviously bayes theorem describes the way in which one s assumptions about observing the event a are changed by having observed the event b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure  the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization programming in this sense means finding an acceptable plan of action an algorithm  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem  the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time  to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved  in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance  dynamic programming makes use of  overlapping subproblems optimal substructure memoization dynamic programming usually takes one of two approaches  top down approach bottom up approach  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization and cognitive economy less information needs to be stored about each specific entity only its particularities  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "since the develop of the web 2 0 google as one of the most popular search engine in the world  there are many algorithms in the web search accordingly implementations of link analysis algorithms will typical discount such internal links the word computer can be exploited by web search engines such as google thus the web is just like a graph and the pagerank which is our first technique for analysing the link which is assigns to every node in the web graph a numerical score between 0 and 1 since the pagerank is the most important algorithms which is used in the google engine for example there are four pages group a b c and d if every page link to a then a s pagerank value shoule be the total value of b c and d  pr a  pr b  pr c  pr d moreover there is a q  0 15 which is be use in the web page like the general algorithm below  however the disadvantage is of pagerank algorithm is that the renew system is too slow \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the vector space model are the documents which are represented as bags of words the basic idea is to represent each document as a vector of certain weighted word frequencies in order to do so the following parsing and extraction steps are needed  1 ignoring case extract all unique words from the entire set of documents  2 eliminate non content bearing  stopwords  such as  a and the  etc for sample lists of stopwords 3 for each document count the number of occurrences of each word  4 using heuristic or information theoretic criteria eliminate non content bearing  high frequency  and  low frequency  words  5 after the above elimination suppose unique words remain assign a unique identifier between and to each remaining word and a unique identifier between and to each document  \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "in probability theory bayes theorem relates the conditional and marginal probabilities of two random events it is usually be used to compute posterior probabilities given observations for instance a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications the articles on bayesian probability and frequentist probability discuss these debates in greater detail frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole at the same time  bayesians describe probabilities in terms of beliefs and degrees of uncertainty  bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b p b a is the conditional probability of b given a p b is the prior or marginal probability of b and acts as a normalizing constant  intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "in mathematics and computer science dynamic programming is a methodology of the solution of the problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the methodology takes much less time rather than naive methods the term was originally used during the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programmer which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming in general  and instead of this it comes from the term mathematical programming  a synonym for optimization therefore the program is the optimal plan for action that is produced for example a finalized schedule of events at an exhibition is sometimes called a program  optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for instance the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices after this it is using this to pick the best overall path in a word we can solve a problem with optimal substructure using a three step process \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "the idea of inheritance in oop refers to the formation of new classes with the already existing classes the concept of inheritance was basically formulated for simula in 1967 as a result the newly created inherited or derived classes inherit the properties and behavior of the classes from which they are derived these original classes are either called base classes or sometimes referred to as ancestor classes the idea of inheritance is to reuse the existing code with little or no modification at all the basic support provided by inheritance is that it represents by categorization in computer languages the power mechanism number of information processing that is crucial to human learning by the means of generalization and cognitive economy is called categorization where generalization if the knowledge of specific entities and is applied to a wider group provided that belongs relation can be created on the other hand cognitive economy is where less information needs to be stored about each specific entity except for some particularities there are examples where we can have modules with similar interfaces the advantage that inheritance provides is that it makes such modules share a lot of code which consequently reduces the complexity of the program \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerank is a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page  it is assumed in several research papers that the distribution is evenly divided between all documents in the collection at the beginning of the computational process pagerank can be calculated for collections of documents of any size the pagerank computations require several passes called iterations  through the collection to adjust approximate pagerank values to more closely reflect the theoretical true value a probability is expressed as a numeric value between 0 and 1 a 0 5 probability is commonly expressed as a 50 chance of something happening hence a pagerank of 0 5 means there is a 50 chance that a person clicking on a random link will be directed to the document with the 0 5 pagerank simplified algorithm how pagerank works assume a small universe of four web pages a b c and d the initial approximation of pagerank would be evenly divided between these four documents hence each document would begin with an estimated pagerank of 0 25 in the original form of pagerank initial values were simply 1 this meant that the sum of all pages was the total number of pages on the web later versions of pagerank see the below formulas would assume a probability distribution between 0 and 1 here we re going to simply use a probability distribution hence the initial value of 0 25 \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "in vector space model the documents from which the information is to be retrieved are represented as vectors the term weighting indentifies the success or failure of the vector space method terms are basically the words or any indexing unit used to identify the contents of a text furthermore a term weighting scheme plays an important role for the similarity measure the similarity measures largely identify the retrieval efficiency of a particular information retrieval system  this largely depends on formulas where the formulas depend only on the frequencies within the document and they not depend on inter document frequencies the main components of the formulas are as follows binary binary formula gives every word that appears in a document equal relevance this can be useful when the number of times a word appears is not considered important term frequency this formula counts how many times the term occurs in a document the more times a term t occurs in document d the more likely it is that t is relevant to the document used alone favors common words and long documents this formula gives more credit to words that appears more frequently but often too much credit augmented normalized term frequency this formula tries to give credit to any word that appears and then give some additional credit to words that appear frequently logarithmic term frequency logarithms are a way to de emphasize the e ect of frequency literature proposes log and alternate log as the most used \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      "bayes theorem is a simple mathematical formula used for calculating conditional probabilities bayes theorem is a theorem of probability theory originally stated by the reverend thomas bayes it figures prominently in subjectivist or bayesian approaches to epistemology statistics and inductive logic it can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence it has been used in a wide variety of contexts ranging from marine biology to the development of bayesian spam blockers for email systems in the philosophy of science it has been used to try to clarify the relationship between theory and evidence many insights in the philosophy of science involving confirmation falsification the relation between science and pseudosience and other topics can be made more precise and sometimes extended or corrected by using bayes theorem subjectivists who maintain that rational belief is governed by the laws of probability lean heavily on conditional probabilities in their theories of evidence and their models of empirical learning bayes theorem is central to these enterprises both because it simplifies the calculation of conditional probabilities and because it clarifies significant features of subjectivist position indeed the theorem s central insight that a hypothesis is confirmed by any body of data that its truth renders probable is the cornerstone of all subjectivist methodology \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      "dynamic programming is a method of providing solutions to potential problems exhibiting the properties of overlapping sub problems and optimal structure this is highly used in dynamic programming the advantage being the less time consumption in comparison to other amateur methods it has to be kept in mind that the term programming in the field has got nothing to do with computer programming at all on the other hand it is derived from the term mathematical programming which is a similar word used for optimization here by meaning that a program can be an optimal plan for the produced action the typical example could be of a finalized schedule of events at an exhibition this leads to the concept of programming being a helper in finding an acceptable plan of action which can also be termed as an algorithm the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time overlapping subproblems means that the same subproblems are used to solve many different larger problems example could be of fibonacci sequence f3  f1  f2 and f4  f2  f3 computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more it means that whenever we encounter with overlapping subproblems a naive approach may waste to e recomputing optimal solutions to the already solved subproblems \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "a\n",
      "object oriented programming is a style of programming that supports encapsulation inheritance and polymorphism inheritance means derived a new class from the base class  we can also say there are parents class and child classes in inheritance  inheritance was firstly derived in 1967 the child class has all the features of parents class or we can say the base class more over it may also include some additional features  inheritance is used for modification and implementation new features in computer programming language it is possible that child class has all the attributes of parents class but it is not possible that all the attributes of child class must have in base class or parent class i categorization in computer language also inheritance is a useful tool categorization define as a powerful feature it has been also used in generalisation and in human learning in some areas less information need to be stored generlisation also some time known as inheritance the main reason behind this is a hierarchi structure of objects and classes we can understand this mechanism by some examples like fruit is aq main class and mangoes apple orange is child classs of the main class so obviously inherit all the properties of fruit class \n",
      "in object oriented programming inheritance is a way to form new classes instances of which are called objects using classes that have already been defined the inheritance concept was invented in 1967 for simula  the new classes known as derived classes take over or inherit attributes and behavior of the pre existing classes which are referred to as base classes or ancestor classes  it is intended to help reuse existing code with little or no modification  inheritance provides the support for representation by categorization in computer languages categorization is a powerful mechanism number of information processing crucial to human learning by means of generalization what is known about specific entities is applied to a wider group given a belongs relation can be established and cognitive economy less information needs to be stored about each specific entity only its particularities  inheritance is also sometimes called generalization because the is a relationships represent a hierarchy between classes of objects for instance a fruit is a generalization of apple  orange  mango and many others one can consider fruit to be an abstraction of apple orange etc conversely since apples are fruit i e  an apple is a fruit  apples may naturally inherit all the properties common to all fruit such as being a fleshy container for the seed of a plant  an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code reducing the complexity of the program inheritance therefore has another view a dual called polymorphism which describes many pieces of code being controlled by shared control code inheritance is typically accomplished either by overriding replacing one or more methods exposed by ancestor or by adding new methods to those exposed by an ancestor  complex inheritance or inheritance used within a design that is not sufficiently mature may lead to the yo yo problem \n",
      "b\n",
      "pagerankalgorithm is also known as link analysis algorithm  it has been used by google the algorithm may be applied to any collection of entities with reciprocal quotations and hyperlinked set of documents such as the world wide web with the purpose of measuring references the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university  in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links numerous academic papers concerning pagerank have been published since page and brin s original paper 4 in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank  \n",
      "pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents such as the world wide web with the purpose of measuring its relative importance within the set the algorithm may be applied to any collection of entities with reciprocal quotations and references the numerical weight that it assigns to any given element e is also called the pagerank of e and denoted by pr e  the name pagerank is a trademark of google and the pagerank process has been patented u s patent 6 285 999  however the patent is assigned to stanford university and not to google google has exclusive license rights on the patent from stanford university the university received 1 8 million shares in google in exchange for use of the patent the shares were sold in 2005 for 336 million google describes pagerank pagerank relies on the uniquely democratic nature of the web by using its vast link structure as an indicator of an individual page s value in essence google interprets a link from page a to page b as a vote by page a for page b but google looks at more than the sheer volume of votes or links a page receives it also analyzes the page that casts the vote votes cast by pages that are themselves important weigh more heavily and help to make other pages important in other words a pagerank results from a ballot among all the other pages on the world wide web about how important a page is a hyperlink to a page counts as a vote of support the pagerank of a page is defined recursively and depends on the number and pagerank metric of all pages that link to it  incoming links  a page that is linked to by many pages with high pagerank receives a high rank itself if there are no links to a web page there is no support for that page google assigns a numeric weighting from 0 10 for each webpage on the internet this pagerank denotes a site s importance in the eyes of google the pagerank is derived from a theoretical probability value on a logarithmic scale like the richter scale the pagerank of a particular page is roughly based upon the quantity of inbound links as well as the pagerank of the pages providing the links it is known that other factors e g relevance of search words on the page and actual visits to the page reported by the google toolbar also influence the pagerank in order to prevent manipulation spoofing and spamdexing google provides no specific details about how other factors influence pagerank numerous academic papers concerning pagerank have been published since page and brin s original paper in practice the pagerank concept has proven to be vulnerable to manipulation and extensive research has been devoted to identifying falsely inflated pagerank and ways to ignore links from documents with falsely inflated pagerank other link based ranking algorithms for web pages include the hits algorithm invented by jon kleinberg used by teoma and now ask com  the ibm clever project and the trustrank algorithm \n",
      "c\n",
      "the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary a document is represented as a vector each dimensions corresponds to a separate terms if a term occurs in the document its value in the vector is non zero relevancy rankings of documents in a keyword search can be calculated using the assumptions of document similarities theory by comparing the deviation of angles between each document vector and the original query vector where the query is represented as same kind of vector as the documents limitation there is some limitation of vector space model models based on and extending the vector space model include  generalized vector space model  enhanced topic based vector space model 1  etvsm  extends the vector space model by removing the constraint that the term vectors be orthogonal in contrast to the generalized vector space model the enhanced topic based vector space model does not depend on concurrence based similarities between terms the enhancement of the enhanced topic based vector space model  compared to the not enhanced one is a proposal on how to derive term vectors from an ontology \n",
      "vector space model or term vector model is an algebraic model for representing text documents and any objects in general as vectors of identifiers such as for example index terms it is used in information filtering information retrieval indexing and relevancy rankings its first use was in the smart information retrieval system a document is represented as a vector each dimension corresponds to a separate term if a term occurs in the document its value in the vector is non zero several different ways of computing these values also known as term weights have been developed one of the best known schemes is tf idf weighting see the example below  the definition of term depends on the application typically terms are single words keywords or longer phrases if the words are chosen to be the terms the dimensionality of the vector is the number of words in the vocabulary the number of distinct words occurring in the corpus  the vector space model has the following limitations 1 long documents are poorly represented because they have poor similarity values a small scalar product and a large dimensionality 2 search keywords must precisely match document terms word substrings might result in a false positive match 3 semantic sensitivity documents with similar context but different term vocabulary won t be associated resulting in a false negative match 4 the order in which the terms appear in the document is lost in the vector space representation \n",
      "d\n",
      " bayes theorem or bayes rule  or something called bayesian reasoning the bayesian conspiracy is a multinational interdisciplinary and shadowy group of scientists that controls publication grants tenure and the illicit traffic in grad students  the best way to be accepted into the bayesian conspiracy is to join the campus crusade for bayes in high school or college and gradually work your way up to the inner circles bayes theorem  let and be sets conditional probability requires that 1  where denotes intersection  and  and also that 2  therefore 3  now let 4  so is an event in and for  then 5 6  but this can be written 7  so this paper proposes a new measure called scaled inverse document frequency sidf which evaluates the conditional specificity of query terms over a subset s of d and without making any assumption about term independence s can be estimated from search results or searches or computed from inverted index data we have evaluated sidf values from commercial search engines by submitting queries relevant to the financial investment domain results compare favorably across search engines and queries our approach has practical applications for real world scenarios like in web mining homeland security and keyword driven marketing research scenarios  \n",
      "in probability theory bayes theorem often called bayes law after rev thomas bayes relates the conditional and marginal probabilities of two random events it is often used to compute posterior probabilities given observations for example a patient may be observed to have certain symptoms bayes theorem can be used to compute the probability that a proposed diagnosis is correct given that observation  see example 2 as a formal theorem bayes theorem is valid in all common interpretations of probability however it plays a central role in the debate around the foundations of statistics frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole while bayesians describe probabilities in terms of beliefs and degrees of uncertainty the articles on bayesian probability and frequentist probability discuss these debates in greater detail bayes theorem relates the conditional and marginal probabilities of events a and b where b has a non vanishing probability p a b frac p b  a  p a  p b  each term in bayes theorem has a conventional name  p a is the prior probability or marginal probability of a it is prior in the sense that it does not take into account any information about b  p a b is the conditional probability of a given b it is also called the posterior probability because it is derived from or depends upon the specified value of b  p b a is the conditional probability of b given a  p b is the prior or marginal probability of b and acts as a normalizing constant intuitively bayes theorem in this form describes the way in which one s beliefs about observing a are updated by having observed b  \n",
      "e\n",
      " dynamic programming is a method for efficiently solving a broad range of search and optimization problems which exhibit the characteristics of overlappling  dynamic programming design technique like divide and conquer method  the leading and most up to date textbook on the far ranging algorithmic methododogy of dynamic programming which can be used for optimal control the word programming in the name has nothing to do with writing computer programs mathematicians use the word to describe a set of rules which anyone can follow to solve a problem they do not have to be written in a computer language  dynamic programming was the brainchild of an american mathematician richard bellman who described the way of solving problems where you need to find the best decisions one after another in the forty odd years since this development the number of uses and applications of dynamic programming has increased enormously  for example in 1982 david kohler used dynamic programming to analyse the best way to play the game of darts  1 in recent years dynamic programming languages develope very fastly especially php and ruby there is no doubt that they have already became the first choice for many programmerers when developing web applications when you learn a new natural language and you start to use it you naturally you find yourself using new concepts and paradigms that enrich the use of the language you already know expect the same result with computer languages  \n",
      "in mathematics and computer science dynamic programming is a method of solving problems that exhibit the properties of overlapping subproblems and optimal substructure described below  the method takes much less time than naive methods the term was originally used in the 1940s by richard bellman to describe the process of solving problems where one needs to find the best decisions one after another by 1953 he had refined this to the modern meaning the field was founded as a systems analysis and engineering topic that is recognized by the ieee bellman s contribution is remembered in the name of the bellman equation a central result of dynamic programming which restates an optimization problem in recursive form the word programming in dynamic programming has no particular connection to computer programming at all and instead comes from the term mathematical programming  a synonym for optimization thus the program is the optimal plan for action that is produced for instance a finalized schedule of events at an exhibition is sometimes called a program programming in this sense means finding an acceptable plan of action an algorithm optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem for example the shortest path to a goal from a vertex in a graph can be found by first computing the shortest path to the goal from all adjacent vertices and then using this to pick the best overall path as shown in figure 1 in general we can solve a problem with optimal substructure using a three step process 1 break the problem into smaller subproblems 2 solve these problems optimally using this three step process recursively 3 use these optimal solutions to construct an optimal solution for the original problem the subproblems are themselves solved by dividing them into sub subproblems and so on until we reach some simple case that is solvable in constant time figure 2 the subproblem graph for the fibonacci sequence that it is not a tree but a dag indicates overlapping subproblems to say that a problem has overlapping subproblems is to say that the same subproblems are used to solve many different larger problems for example in the fibonacci sequence f3  f1  f2 and f4  f2  f3  computing each number involves computing f2 because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more this applies whenever overlapping subproblems are present a naive approach may waste time recomputing optimal solutions to subproblems it has already solved in order to avoid this we instead save the solutions to problems we have already solved then if we need to solve the same problem later we can retrieve and reuse our already computed solution this approach is called memoization not memorization although this term also fits  if we are sure we won t need a particular solution anymore we can throw it away to save space in some cases we can even compute the solutions to subproblems we know that we ll need in advance \n",
      "6-gram containment features created!\n",
      "LCS features created!\n",
      "\n",
      "Features:  ['c_1', 'c_2', 'c_3', 'c_4', 'c_5', 'c_6', 'lcs_word']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an ngram range\n",
    "ngram_range = range(1,7)\n",
    "\n",
    "\n",
    "# The following code may take a minute to run, depending on your ngram_range\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "features_list = []\n",
    "\n",
    "# Create features in a features_df\n",
    "all_features = np.zeros((len(ngram_range)+1, len(complete_df)))\n",
    "\n",
    "# Calculate features for containment for ngrams in range\n",
    "i=0\n",
    "for n in ngram_range:\n",
    "    column_name = 'c_'+str(n)\n",
    "    features_list.append(column_name)\n",
    "    # create containment features\n",
    "    all_features[i]=np.squeeze(create_containment_features(complete_df, n))\n",
    "    i+=1\n",
    "\n",
    "# Calculate features for LCS_Norm Words \n",
    "features_list.append('lcs_word')\n",
    "all_features[i]= np.squeeze(create_lcs_features(complete_df))\n",
    "\n",
    "# create a features dataframe\n",
    "features_df = pd.DataFrame(np.transpose(all_features), columns=features_list)\n",
    "\n",
    "# Print all features/columns\n",
    "print()\n",
    "print('Features: ', features_list)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>lcs_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984694</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>0.943299</td>\n",
       "      <td>0.922280</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>0.820755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.869369</td>\n",
       "      <td>0.719457</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.515982</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.382488</td>\n",
       "      <td>0.846491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.593583</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.156757</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.060440</td>\n",
       "      <td>0.316062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.329502</td>\n",
       "      <td>0.053846</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.150442</td>\n",
       "      <td>0.035556</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.709898</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.625430</td>\n",
       "      <td>0.589655</td>\n",
       "      <td>0.553633</td>\n",
       "      <td>0.621711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.395480</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.245714</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.484305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>0.340807</td>\n",
       "      <td>0.247748</td>\n",
       "      <td>0.180995</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.597458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        c_1       c_2       c_3       c_4       c_5       c_6  lcs_word\n",
       "0  0.398148  0.079070  0.009346  0.000000  0.000000  0.000000  0.191781\n",
       "1  1.000000  0.984694  0.964103  0.943299  0.922280  0.901042  0.820755\n",
       "2  0.869369  0.719457  0.613636  0.515982  0.449541  0.382488  0.846491\n",
       "3  0.593583  0.268817  0.156757  0.108696  0.081967  0.060440  0.316062\n",
       "4  0.544503  0.115789  0.031746  0.005319  0.000000  0.000000  0.242574\n",
       "5  0.329502  0.053846  0.007722  0.003876  0.000000  0.000000  0.161172\n",
       "6  0.590308  0.150442  0.035556  0.004464  0.000000  0.000000  0.301653\n",
       "7  0.765306  0.709898  0.664384  0.625430  0.589655  0.553633  0.621711\n",
       "8  0.759777  0.505618  0.395480  0.306818  0.245714  0.195402  0.484305\n",
       "9  0.884444  0.526786  0.340807  0.247748  0.180995  0.150000  0.597458"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print some results \n",
    "features_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Features\n",
    "\n",
    "You should use feature correlation across the *entire* dataset to determine which features are ***too*** **highly-correlated** with each other to include both features in a single model. For this analysis, you can use the *entire* dataset due to the small sample size we have. \n",
    "\n",
    "All of our features try to measure the similarity between two texts. Since our features are designed to measure similarity, it is expected that these features will be highly-correlated. Many classification models, for example a Naive Bayes classifier, rely on the assumption that features are *not* highly correlated; highly-correlated features may over-inflate the importance of a single feature. \n",
    "\n",
    "So, you'll want to choose your features based on which pairings have the lowest correlation. These correlation values range between 0 and 1; from low to high correlation, and are displayed in a [correlation matrix](https://www.displayr.com/what-is-a-correlation-matrix/), below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>c_3</th>\n",
       "      <th>c_4</th>\n",
       "      <th>c_5</th>\n",
       "      <th>c_6</th>\n",
       "      <th>lcs_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c_1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_2</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_4</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_5</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_6</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lcs_word</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           c_1   c_2   c_3   c_4   c_5   c_6  lcs_word\n",
       "c_1       1.00  0.94  0.90  0.89  0.88  0.87      0.97\n",
       "c_2       0.94  1.00  0.99  0.98  0.97  0.96      0.98\n",
       "c_3       0.90  0.99  1.00  1.00  0.99  0.98      0.97\n",
       "c_4       0.89  0.98  1.00  1.00  1.00  0.99      0.95\n",
       "c_5       0.88  0.97  0.99  1.00  1.00  1.00      0.95\n",
       "c_6       0.87  0.96  0.98  0.99  1.00  1.00      0.94\n",
       "lcs_word  0.97  0.98  0.97  0.95  0.95  0.94      1.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Create correlation matrix for just Features to determine different models to test\n",
    "corr_matrix = features_df.corr().abs().round(2)\n",
    "\n",
    "# display shows all of a dataframe\n",
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Create selected train/test data\n",
    "\n",
    "Complete the `train_test_data` function below. This function should take in the following parameters:\n",
    "* `complete_df`: A DataFrame that contains all of our processed text data, file info, datatypes, and class labels\n",
    "* `features_df`: A DataFrame of all calculated features, such as containment for ngrams, n= 1-5, and lcs values for each text file listed in the `complete_df` (this was created in the above cells)\n",
    "* `selected_features`: A list of feature column names,  ex. `['c_1', 'lcs_word']`, which will be used to select the final features in creating train/test sets of data.\n",
    "\n",
    "It should return two tuples:\n",
    "* `(train_x, train_y)`, selected training features and their corresponding class labels (0/1)\n",
    "* `(test_x, test_y)`, selected training features and their corresponding class labels (0/1)\n",
    "\n",
    "** Note: x and y should be arrays of feature values and numerical class labels, respectively; not DataFrames.**\n",
    "\n",
    "Looking at the above correlation matrix, you should decide on a **cutoff** correlation value, less than 1.0, to determine which sets of features are *too* highly-correlated to be included in the final training and test data. If you cannot find features that are less correlated than some cutoff value, it is suggested that you increase the number of features (longer n-grams) to choose from or use *only one or two* features in your final model to avoid introducing highly-correlated features.\n",
    "\n",
    "Recall that the `complete_df` has a `Datatype` column that indicates whether data should be `train` or `test` data; this should help you split the data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in dataframes and a list of selected features (column names) \n",
    "# and returns (train_x, train_y), (test_x, test_y)\n",
    "def train_test_data(complete_df, features_df, selected_features):\n",
    "    '''Gets selected training and test features from given dataframes, and \n",
    "       returns tuples for training and test features and their corresponding class labels.\n",
    "       :param complete_df: A dataframe with all of our processed text data, datatypes, and labels\n",
    "       :param features_df: A dataframe of all computed, similarity features\n",
    "       :param selected_features: An array of selected features that correspond to certain columns in `features_df`\n",
    "       :return: training and test features and labels: (train_x, train_y), (test_x, test_y)'''\n",
    "    \n",
    "    df_new = pd.concat([complete_df,features_df], axis =1)\n",
    "    \n",
    "    train = df_new[df_new['Datatype']=='train']\n",
    "    test = df_new[df_new['Datatype']=='test']\n",
    "\n",
    "    \n",
    "    # get the training features\n",
    "    train_x = train[selected_features].values\n",
    "    # And training class labels (0 or 1)\n",
    "    train_y = train['Class'].values\n",
    "    \n",
    "    # get the test features and labels\n",
    "    test_x = test[selected_features].values\n",
    "    test_y = test['Class'].values\n",
    "    \n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "Below, test out your implementation and create the final train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "test_selection = list(features_df)[:2] # first couple columns as a test\n",
    "# test that the correct train/test data is created\n",
    "(train_x, train_y), (test_x, test_y) = train_test_data(complete_df, features_df, test_selection)\n",
    "\n",
    "# params: generated train/test data\n",
    "tests.test_data_split(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Select \"good\" features\n",
    "\n",
    "If you passed the test above, you can create your own train/test data, below. \n",
    "\n",
    "Define a list of features you'd like to include in your final mode, `selected_features`; this is a list of the features names you want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  70\n",
      "Test size:  25\n",
      "\n",
      "Training df sample: \n",
      " [[0.39814815 0.         0.19178082]\n",
      " [0.86936937 0.44954128 0.84649123]\n",
      " [0.59358289 0.08196721 0.31606218]\n",
      " [0.54450262 0.         0.24257426]\n",
      " [0.32950192 0.         0.16117216]\n",
      " [0.59030837 0.         0.30165289]\n",
      " [0.75977654 0.24571429 0.48430493]\n",
      " [0.51612903 0.         0.27083333]\n",
      " [0.44086022 0.         0.22395833]\n",
      " [0.97945205 0.78873239 0.9       ]]\n"
     ]
    }
   ],
   "source": [
    "# Select your list of features, this should be column names from features_df\n",
    "# ex. ['c_1', 'lcs_word']\n",
    "selected_features = ['c_1', 'c_5', 'lcs_word']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = train_test_data(complete_df, features_df, selected_features)\n",
    "\n",
    "# check that division of samples seems correct\n",
    "# these should add up to 95 (100 - 5 original files)\n",
    "print('Training size: ', len(train_x))\n",
    "print('Test size: ', len(test_x))\n",
    "print()\n",
    "print('Training df sample: \\n', train_x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: How did you decide on which features to include in your final model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**The pairs which have correlations 1 can be taken into consideration like (c4,c3) an (c5,c4) and (c5,c6).\n",
    "Since, c4 =c3, c4 = c5, we can ignore c4 if we take c3. And we can ignore c5 if we take c4 and c6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating Final Data Files\n",
    "\n",
    "Now, you are almost ready to move on to training a model in SageMaker!\n",
    "\n",
    "You'll want to access your train and test data in SageMaker and upload it to S3. In this project, SageMaker will expect the following format for your train/test data:\n",
    "* Training and test data should be saved in one `.csv` file each, ex `train.csv` and `test.csv`\n",
    "* These files should have class  labels in the first column and features in the rest of the columns\n",
    "\n",
    "This format follows the practice, outlined in the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html), which reads: \"Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable [class label] is in the first column.\"\n",
    "\n",
    "## EXERCISE: Create csv files\n",
    "\n",
    "Define a function that takes in x (features) and y (labels) and saves them to one `.csv` file at the path `data_dir/filename`.\n",
    "\n",
    "It may be useful to use pandas to merge your features and labels into one DataFrame and then convert that into a csv file. You can make sure to get rid of any incomplete rows, in a DataFrame, by using `dropna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    # make data dir, if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df = pd.concat([pd.Series(y),pd.DataFrame(x)], axis=1).dropna()\n",
    "    \n",
    "    df.to_csv(os.path.join(data_dir,filename),header= None, index = None)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cells\n",
    "\n",
    "Test that your code produces the correct format for a `.csv` file, given some text features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: test_csv/to_delete.csv\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "fake_x = [ [0.39814815, 0.0001, 0.19178082], \n",
    "           [0.86936937, 0.44954128, 0.84649123], \n",
    "           [0.44086022, 0., 0.22395833] ]\n",
    "\n",
    "fake_y = [0, 1, 1]\n",
    "\n",
    "make_csv(fake_x, fake_y, filename='to_delete.csv', data_dir='test_csv')\n",
    "\n",
    "# read in and test dimensions\n",
    "fake_df = pd.read_csv('test_csv/to_delete.csv', header=None)\n",
    "\n",
    "# check shape\n",
    "assert fake_df.shape==(3, 4), \\\n",
    "      'The file should have as many rows as data_points and as many columns as features+1 (for indices).'\n",
    "# check that first column = labels\n",
    "assert np.all(fake_df.iloc[:,0].values==fake_y), 'First column is not equal to the labels, fake_y.'\n",
    "print('Tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the test csv file, generated above\n",
    "! rm -rf test_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've passed the tests above, run the following cell to create `train.csv` and `test.csv` files in a directory that you specify! This will save the data in a local directory. Remember the name of this directory because you will reference it again when uploading this data to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: plagiarism_data/train.csv\n",
      "Path created: plagiarism_data/test.csv\n"
     ]
    }
   ],
   "source": [
    "# can change directory, if you want\n",
    "data_dir = 'plagiarism_data'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "make_csv(train_x, train_y, filename='train.csv', data_dir=data_dir)\n",
    "make_csv(test_x, test_y, filename='test.csv', data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "Now that you've done some feature engineering and created some training and test data, you are ready to train and deploy a plagiarism classification model. The next notebook will utilize SageMaker resources to train and test a model that you design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
